{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which GPU to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiGPU = False\n",
    "whichGPU = 0\n",
    " \n",
    "# Select which GPU to use\n",
    "if(multiGPU):\n",
    "    from keras.utils.training_utils import multi_gpu_model\n",
    "else:\n",
    "    import os\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "    # The GPU id to use, usually either \"0\" or \"1\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(whichGPU)\n",
    "    \n",
    "# # Do other imports now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%run -i 'arena.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# What data to use\n",
    "tableBase = '4PpKk'\n",
    "convertStates = False\n",
    "\n",
    "# Interactive (just in general if one is asked for confirmations, set to False if on autopilot over night f.x.)\n",
    "askForConfirmation = False\n",
    "\n",
    "# NN parameters\n",
    "filters = [16,32,32,64,128,128,128]\n",
    "filterShape = [2,2,2,2,2,2,2]\n",
    "batch_size = 256\n",
    "optimizer = 'Adadelta'\n",
    "useBatchNorm = False\n",
    "num_classes = 3\n",
    "input_shape = (4,8,8)\n",
    "\n",
    "### DON'T MODIFY BELOW ###\n",
    "# Generate dataset variables\n",
    "fileName = tableBase + '.hdf5'\n",
    "dataSetName = tableBase + '_onlyLegal'\n",
    "if not convertStates: \n",
    "    dataSetName = tableBase + '_onlyLegal_fullStates'\n",
    "dataSetWdlName = tableBase + '_Wdl_onlyLegal_3Values'\n",
    "\n",
    "# Number of Pieces\n",
    "nPi =  int(dataSetName[0])\n",
    "nPa = nPi - 2\n",
    "nWPa = math.ceil(nPa/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 0\n",
    "test batch size vs time vs validation valley location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp 0 Paramters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%run -i 'arena.py'\n",
    "\n",
    "# Parameters\n",
    "weightsSource = '103' # trained on 3pc from scratch\n",
    "# sourceNet = '103' # trained on 3pc from scratch\n",
    "# sourceNet = '107' # trained on 4pc from scratch\n",
    "# freeze = False\n",
    "epochs = 350\n",
    "expDescrBaseName = \"Batch size test\"\n",
    "\n",
    "saveEveryRun = True # save stuff in results dir\n",
    "saveWeightsCheckpoints = False # save chkp in results dit\n",
    "saveTensorboardLogs = True # save logs in ./logs dir\n",
    "# resID = '---NORESID---' # used when not saving data, but fitModel() still needs a resID\n",
    "\n",
    "fractionOfDataToUse = 1\n",
    "plotDuringTraining = False\n",
    "loadWeights = False \n",
    "loadCheckpointWeights = False\n",
    "askForConfirmation = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model and load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# load data\n",
    "X_train, X_test, y_train, y_test = loadData(randomState = None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "%run -i 'arena.py'\n",
    "\n",
    "batches = [128, 1024]\n",
    "for batch_size in batches:\n",
    "    \n",
    "    expDescr = expDescrBaseName + '_batchSize{}'.format(batch_size)\n",
    "    \n",
    "    # create model\n",
    "    model, nnStr = createModel()\n",
    "\n",
    "    # Prepare save dir\n",
    "    if saveEveryRun:\n",
    "        resID = genNextResultsDir(model)\n",
    "        \n",
    "    # train\n",
    "    fitHistory, logDir = trainModel(resID, model, saveWeightsCheckpoints, saveTensorboardLogs)\n",
    "        \n",
    "    # score and save results\n",
    "    score = calcScore(model)\n",
    "    if saveEveryRun:\n",
    "        saveTrainResults(resID, model, logDir, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1\n",
    "Bengio methood\n",
    "3n4 no freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp 1 Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3n4 no freeze \n",
    "%run -i 'arena.py'\n",
    "\n",
    "# Parameters\n",
    "sourceNet = '103' # trained on 3pc from scratch\n",
    "# sourceNet = '107' # trained on 4pc from scratch\n",
    "freeze = False\n",
    "epochs = 10\n",
    "averageOver = 10\n",
    "expDescrBaseName = \"Bengio 3n4 - freeze = {} - average over {} runs - {} epochs\".format(str(freeze), averageOver, epochs)\n",
    "\n",
    "saveEveryRun = True # save stuff in results dir\n",
    "saveWeightsCheckpoints = False # save chkp in results dit\n",
    "saveTensorboardLogs = True # save logs in ./logs dir\n",
    "resID = '---NORESID---' # used when not saving data, but fitModel() still needs a resID\n",
    "\n",
    "fractionOfDataToUse = 1\n",
    "plotDuringTraining = False\n",
    "loadWeights = False \n",
    "askForConfirmation = False\n",
    "\n",
    "# stuff to save in bengioResults dir\n",
    "resSaveFile = '3n4nofreeze'\n",
    "saveDir = 'bengioResults'\n",
    "resSaveFile = resSaveFile + '-{}runAverage-{}epochs'.format(averageOver, epochs)\n",
    "resSaveFileFullPath = saveDir + '/' + str(resSaveFile) + '.pkl' \n",
    "resSaveFileAllRuns = resSaveFile + '-allRuns'.format(averageOver, epochs)\n",
    "resSaveFileAllRunsPath = saveDir + '/' + str(resSaveFileAllRuns) + '.pkl' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save file exists...\n",
      "\n",
      "7436088\n",
      "7436088\n",
      "X_train shape: (4982178, 4, 8, 8)\n",
      "y_train shape: (4982178, 1)\n",
      "X_test shape: (2453910, 4, 8, 8)\n",
      "y_test shape: (2453910, 1)\n",
      "4982178 train samples\n",
      "2453910 test samples\n",
      "Done loading dataset\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 16, 7, 7)          272       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 6, 32)         928       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 5, 32)         4128      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 4, 64)         8256      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 3, 128)        32896     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 11, 2, 128)        65664     \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 10, 1, 128)        65664     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 3843      \n",
      "=================================================================\n",
      "Total params: 181,651\n",
      "Trainable params: 181,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Starting with random weights\n",
      "Done creating model\n",
      "\n",
      "Starting/restarting TL at 7 transfered layers\n"
     ]
    }
   ],
   "source": [
    "# prepare save file\n",
    "if not os.path.exists(resSaveFileFullPath):\n",
    "    print(\"Save file doesn't exists, creating...\\n\")\n",
    "    save_obj(saveDir, resSaveFile, [])\n",
    "else:\n",
    "    print(\"Save file exists...\\n\")\n",
    "\n",
    "\n",
    "# load data\n",
    "X_train, X_test, y_train, y_test = loadData(randomState = None)\n",
    "\n",
    "# create model\n",
    "model, nnStr = createModel()\n",
    "layersCount = len(model.layers)\n",
    "\n",
    "# load old results\n",
    "results = load_obj(saveDir, resSaveFile)\n",
    "resultsAllRuns = load_obj(saveDir, resSaveFileAllRuns)\n",
    "\n",
    "# initialize variables wrt old results\n",
    "startTrainingAtLayer = len(results)\n",
    "print(\"\\nStarting/restarting TL at {} transfered layers\".format(startTrainingAtLayer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xFull = [[0.9521808053271059, 0.9592193682733443, 0.9589202537988933, 0.9497581410893619, 0.946441393531383, 0.9349295613938086, 0.9273335207892224, 0.9165715124027525, 0.9282198613642059, 0.9141451805485453], [0.9195867004089298, 0.9203426368531195, 0.9182655435611895, 0.924165922955561, 0.9087440044661471, 0.9116760598392926, 0.919438773223239, 0.9176807625382506, 0.9089722116946111, 0.8977436010287569], [0.9217379610497219, 0.9125562877202192, 0.9177936436134672, 0.9171090219280753, 0.9188059056770137, 0.9123052597692003, 0.9242804340828468, 0.9265894022192667, 0.925673720715055, 0.921387499949158], [0.9350461100854884, 0.9342445321956544, 0.9364524371307518, 0.940469291864917, 0.9395140816087778, 0.940754550900497, 0.9480628873920554, 0.9427582918691378, 0.9402614602818515, 0.945601916940962], [0.9557881095882255, 0.9594487980409209, 0.9589080284117999, 0.9621176000748366, 0.9630789230247411, 0.962064623397189, 0.9619774156348831, 0.9603078352507787, 0.9621620189818055, 0.9632040294874883], [0.9704019299813751, 0.9716293588597962, 0.971290715633272, 0.9670175352806729, 0.9704194530360292, 0.9715441886621758, 0.9722194375505386, 0.9739921187003598, 0.9725022515088165, 0.9721790937730381], [0.9762961966822729, 0.977575379700152, 0.9756506962358032, 0.9786235028994543, 0.9761446018805074, 0.9773732532977982, 0.9764803925164329, 0.9786540663675521, 0.9790546515559423, 0.978106369019093]]\n",
    "xFullConv =  convertFullToMeanError(xFull)\n",
    "x = [0.9387719598518622, 0.9146616216569097, 0.9198239136724025, 0.9403165560270093, 0.9609057381892668, 0.9713196082986075, 0.9773959110155008]\n",
    "xFullConv[:,0] - x\n",
    "save_obj('.','xFull', xFull)\n",
    "save_obj('.','x', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "=                                                                                        =\n",
      "=                    Currently transfering first 7 layers, out of 8                      =\n",
      "=                                                                                        =\n",
      "==========================================================================================\n",
      "\n",
      "- 8: Skipping layer <keras.layers.core.Flatten object at 0x7f35adaddf98>\n",
      "- 9: Resetting layer <keras.layers.core.Dense object at 0x7f35adaf6e10>\n",
      "Save dir: Results/221/\n",
      "Creating save dir\n",
      "Done generating results dir Results/221/\n",
      "Train on 4982178 samples, validate on 2453910 samples\n",
      "Epoch 1/10\n",
      "1440768/4982178 [=======>......................] - ETA: 1:49 - loss: 0.4090 - acc: 0.8202"
     ]
    }
   ],
   "source": [
    "%run -i 'arena.py'\n",
    "\n",
    "resultsAllRuns = []\n",
    "\n",
    "for copyFirstNLayers in range(startTrainingAtLayer, layersCount):\n",
    "    print('\\n\\n')\n",
    "    print('==========================================================================================')\n",
    "    print('=                                                                                        =')\n",
    "    print('=                    Currently transfering first {} layers, out of {}                      ='.format(copyFirstNLayers, layersCount - 1))\n",
    "    print('=                                                                                        =')\n",
    "    print('==========================================================================================')\n",
    "    print()\n",
    "    \n",
    "    # check if we are at the flatten layer, and skip it if so\n",
    "    if copyFirstNLayers == layersCount - 1:\n",
    "        copyFirstNLayers += 1\n",
    "        \n",
    "    # train and average results\n",
    "    \n",
    "    accumulatedScore = 0\n",
    "    resultsThisRun = []\n",
    "    for a in range(averageOver):\n",
    "        \n",
    "        # set experement description test\n",
    "        expDescr = expDescrBaseName + '__copyLayers_{}__average_{}_of_{}'.format(copyFirstNLayers, a+1, averageOver)\n",
    "        \n",
    "        # save current averagePosition to tmp file\n",
    "        with open(saveDir + '/' + str(resSaveFile) + '_currentPosition.txt','w') as file:\n",
    "            if copyFirstNLayers == layersCount:\n",
    "                location = copyFirstNLayers - 1\n",
    "            else:\n",
    "                location = copyFirstNLayers \n",
    "            file.write('Layers Transfered: {} out of {} \\nInner avg loop position: {} out of {}'.format(location, layersCount-1, a+1, averageOver))         \n",
    "            \n",
    "        # load Model layers\n",
    "        model = loadNFirstLayers(model, sourceNet, copyFirstNLayers , freeze)\n",
    "\n",
    "        # Prepare save dir\n",
    "        if saveEveryRun:\n",
    "            resID = genNextResultsDir(model)\n",
    "\n",
    "        # train\n",
    "        fitHistory, logDir = trainModel(resID, model, saveWeightsCheckpoints, saveTensorboardLogs)\n",
    "\n",
    "        # score and save results\n",
    "        score = calcScore(model)\n",
    "        if saveEveryRun:\n",
    "            saveTrainResults(resID, model, logDir, score, copyFirstNLayers)\n",
    "\n",
    "        # update Return\n",
    "        accumulatedScore += score[1]\n",
    "        resultsThisRun.append(score[1])\n",
    "        \n",
    "    # append averaged results for one set of layers\n",
    "    results.append(accumulatedScore/averageOver)\n",
    "    resultsAllRuns.append(resultsThisRun)\n",
    "\n",
    "    # save old results to checkpoints dir\n",
    "    dateTime = time.strftime('%Y-%m-%d-%H:%M:%S', time.localtime())\n",
    "    \n",
    "    src = saveDir + '/' + str(resSaveFile) + '.txt'\n",
    "    dest = saveDir + '/checkpoints/' + str(resSaveFile) + dateTime + '.txt'\n",
    "    if os.path.exists(src):\n",
    "        shutil.move(src, dest)\n",
    "\n",
    "    src = saveDir + '/' + str(resSaveFileAllRuns) + '.pkl'\n",
    "    dest = saveDir + '/checkpoints/' + str(resSaveFileAllRuns) + dateTime + '.pkl'\n",
    "    if os.path.exists(src):\n",
    "        shutil.move(src, dest)\n",
    "        \n",
    "    # save results \n",
    "    save_obj(saveDir, resSaveFile, results)\n",
    "    with open(saveDir + '/' + str(resSaveFile) + '.txt','w') as file:\n",
    "        file.write(str(results))\n",
    "    \n",
    "    save_obj(saveDir, resSaveFileAllRuns, resultsAllRuns)\n",
    "    with open(saveDir + '/' + str(resSaveFileAllRuns) + '.txt','w') as file:\n",
    "        file.write(str(resultsAllRuns))\n",
    "\n",
    "    # to load:\n",
    "    # results = load_obj('temp','3n4.txt')\n",
    "print('\\n Final Results: {}'.format(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2\n",
    "Bengio methood\n",
    "4n4 no freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp 2 Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i 'arena.py'\n",
    "\n",
    "# Parameters\n",
    "# sourceNet = '103' # trained on 3pc from scratch\n",
    "sourceNet = '107' # trained on 4pc from scratch ######################## MODIFY\n",
    "freeze = False                           ############################### MODIFY\n",
    "epochs = 10\n",
    "averageOver = 10\n",
    "                          ############################### MODIFY\n",
    "expDescrBaseName = \"Bengio 4n4 - freeze = {} - average over {} runs - {} epochs\".format(str(freeze), averageOver, epochs)\n",
    "\n",
    "saveEveryRun = True # save stuff in results dir\n",
    "saveWeightsCheckpoints = False # save chkp in results dit\n",
    "saveTensorboardLogs = True # save logs in ./logs dir\n",
    "resID = '---NORESID---' # used when not saving data, but fitModel() still needs a resID\n",
    "\n",
    "fractionOfDataToUse = 1\n",
    "plotDuringTraining = False\n",
    "loadWeights = False \n",
    "askForConfirmation = False\n",
    "\n",
    "# stuff to save in bengioResults dir\n",
    "resSaveFile = '4n4nofreeze'                          ############################### MODIFY\n",
    "saveDir = 'bengioResults'\n",
    "resSaveFile = resSaveFile + '-{}runAverage-{}epochs'.format(averageOver, epochs)\n",
    "resSaveFileFullPath = saveDir + '/' + str(resSaveFile) + '.pkl' \n",
    "resSaveFileAllRuns = resSaveFile + '-allRuns'.format(averageOver, epochs)\n",
    "resSaveFileAllRunsPath = saveDir + '/' + str(resSaveFileAllRuns) + '.pkl' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# prepare save file\n",
    "if not os.path.exists(resSaveFileFullPath):\n",
    "    print(\"Save file doesn't exists, creating...\\n\")\n",
    "    save_obj(saveDir, resSaveFile, [])\n",
    "else:\n",
    "    print(\"Save file exists...\\n\")\n",
    "\n",
    "\n",
    "# load data\n",
    "X_train, X_test, y_train, y_test = loadData(randomState = None)\n",
    "\n",
    "# create model\n",
    "model, nnStr = createModel()\n",
    "layersCount = len(model.layers)\n",
    "\n",
    "# load old results\n",
    "results = load_obj(saveDir, resSaveFile)\n",
    "resultsAllRuns = load_obj(saveDir, resSaveFileAllRuns)\n",
    "\n",
    "# initialize variables wrt old results\n",
    "startTrainingAtLayer = len(results)\n",
    "print(\"\\nStarting/restarting TL at {} transfered layers\".format(startTrainingAtLayer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xFull = [[0.9521808053271059, 0.9592193682733443, 0.9589202537988933, 0.9497581410893619, 0.946441393531383, 0.9349295613938086, 0.9273335207892224, 0.9165715124027525, 0.9282198613642059, 0.9141451805485453], [0.9195867004089298, 0.9203426368531195, 0.9182655435611895, 0.924165922955561, 0.9087440044661471, 0.9116760598392926, 0.919438773223239, 0.9176807625382506, 0.9089722116946111, 0.8977436010287569], [0.9217379610497219, 0.9125562877202192, 0.9177936436134672, 0.9171090219280753, 0.9188059056770137, 0.9123052597692003, 0.9242804340828468, 0.9265894022192667, 0.925673720715055, 0.921387499949158], [0.9350461100854884, 0.9342445321956544, 0.9364524371307518, 0.940469291864917, 0.9395140816087778, 0.940754550900497, 0.9480628873920554, 0.9427582918691378, 0.9402614602818515, 0.945601916940962], [0.9557881095882255, 0.9594487980409209, 0.9589080284117999, 0.9621176000748366, 0.9630789230247411, 0.962064623397189, 0.9619774156348831, 0.9603078352507787, 0.9621620189818055, 0.9632040294874883], [0.9704019299813751, 0.9716293588597962, 0.971290715633272, 0.9670175352806729, 0.9704194530360292, 0.9715441886621758, 0.9722194375505386, 0.9739921187003598, 0.9725022515088165, 0.9721790937730381], [0.9762961966822729, 0.977575379700152, 0.9756506962358032, 0.9786235028994543, 0.9761446018805074, 0.9773732532977982, 0.9764803925164329, 0.9786540663675521, 0.9790546515559423, 0.978106369019093]]\n",
    "xFullConv =  convertFullToMeanError(xFull)\n",
    "x = [0.9387719598518622, 0.9146616216569097, 0.9198239136724025, 0.9403165560270093, 0.9609057381892668, 0.9713196082986075, 0.9773959110155008]\n",
    "xFullConv[:,0] - x\n",
    "save_obj('.','xFull', xFull)\n",
    "save_obj('.','x', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run -i 'arena.py'\n",
    "\n",
    "resultsAllRuns = []\n",
    "\n",
    "for copyFirstNLayers in range(startTrainingAtLayer, layersCount):\n",
    "    print('\\n\\n')\n",
    "    print('==========================================================================================')\n",
    "    print('=                                                                                        =')\n",
    "    print('=                    Currently transfering first {} layers, out of {}                      ='.format(copyFirstNLayers, layersCount - 1))\n",
    "    print('=                                                                                        =')\n",
    "    print('==========================================================================================')\n",
    "    print()\n",
    "    \n",
    "    # check if we are at the flatten layer, and skip it if so\n",
    "    if copyFirstNLayers == layersCount - 1:\n",
    "        copyFirstNLayers += 1\n",
    "        \n",
    "    # train and average results\n",
    "    \n",
    "    accumulatedScore = 0\n",
    "    resultsThisRun = []\n",
    "    for a in range(averageOver):\n",
    "        \n",
    "        # set experement description test\n",
    "        expDescr = expDescrBaseName + '__copyLayers_{}__average_{}_of_{}'.format(copyFirstNLayers, a+1, averageOver)\n",
    "        \n",
    "        # save current averagePosition to tmp file\n",
    "        with open(saveDir + '/' + str(resSaveFile) + '_currentPosition.txt','w') as file:\n",
    "            if copyFirstNLayers == layersCount:\n",
    "                location = copyFirstNLayers - 1\n",
    "            else:\n",
    "                location = copyFirstNLayers \n",
    "            file.write('Layers Transfered: {} out of {} \\nInner avg loop position: {} out of {}'.format(location, layersCount-1, a+1, averageOver))         \n",
    "            \n",
    "        # load Model layers\n",
    "        model = loadNFirstLayers(model, sourceNet, copyFirstNLayers , freeze)\n",
    "\n",
    "        # Prepare save dir\n",
    "        if saveEveryRun:\n",
    "            resID = genNextResultsDir(model)\n",
    "\n",
    "        # train\n",
    "        fitHistory, logDir = trainModel(resID, model, saveWeightsCheckpoints, saveTensorboardLogs)\n",
    "\n",
    "        # score and save results\n",
    "        score = calcScore(model)\n",
    "        if saveEveryRun:\n",
    "            saveTrainResults(resID, model, logDir, score, copyFirstNLayers)\n",
    "\n",
    "        # update Return\n",
    "        accumulatedScore += score[1]\n",
    "        resultsThisRun.append(score[1])\n",
    "        \n",
    "    # append averaged results for one set of layers\n",
    "    results.append(accumulatedScore/averageOver)\n",
    "    resultsAllRuns.append(resultsThisRun)\n",
    "\n",
    "    # save old results to checkpoints dir\n",
    "    dateTime = time.strftime('%Y-%m-%d-%H:%M:%S', time.localtime())\n",
    "    \n",
    "    src = saveDir + '/' + str(resSaveFile) + '.txt'\n",
    "    dest = saveDir + '/checkpoints/' + str(resSaveFile) + dateTime + '.txt'\n",
    "    if os.path.exists(src):\n",
    "        shutil.move(src, dest)\n",
    "\n",
    "    src = saveDir + '/' + str(resSaveFileAllRuns) + '.pkl'\n",
    "    dest = saveDir + '/checkpoints/' + str(resSaveFileAllRuns) + dateTime + '.pkl'\n",
    "    if os.path.exists(src):\n",
    "        shutil.move(src, dest)\n",
    "        \n",
    "    # save results \n",
    "    save_obj(saveDir, resSaveFile, results)\n",
    "    with open(saveDir + '/' + str(resSaveFile) + '.txt','w') as file:\n",
    "        file.write(str(results))\n",
    "    \n",
    "    save_obj(saveDir, resSaveFileAllRuns, resultsAllRuns)\n",
    "    with open(saveDir + '/' + str(resSaveFileAllRuns) + '.txt','w') as file:\n",
    "        file.write(str(resultsAllRuns))\n",
    "\n",
    "    # to load:\n",
    "    # results = load_obj('temp','3n4.txt')\n",
    "print('\\n Final Results: {}'.format(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
