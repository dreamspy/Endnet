{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments\n",
    "    - Test if small batch is better\n",
    "        - Exp 1\n",
    "            - Save to 006\n",
    "            - starting from 005, and shring batch size to 512\n",
    "            - Results: after 183 epochs, the accuracy was actually lower than before training. \n",
    "              Batch size is probably not to big at 4096.\n",
    "        - Exp 2\n",
    "            - Starting from 012, a 100.000 pm net trained for 1600 epochs at bs 4096\n",
    "            - Switching to bs 32 created horrible results, network totally failed to perform, lost acc from \n",
    "              0.95 to 0.85 in 17 epochs. Plus, super slow to train\n",
    "                 \n",
    "    - Test Adam\n",
    "        - Results\n",
    "            - 007 Adam, final accuracy 0.939 after 500\n",
    "            - 005 Adadelta, final accuracy 0.952 after 1000 epochs\n",
    "            - Adam not performing better than Adadelta\n",
    "            \n",
    "    - Test 2x2 and 3x3 filters for performance\n",
    "        - 2x2 5 layers (005)\n",
    "        - 3x3 3 layers (003)\n",
    "        - ~50.000 parameters\n",
    "        - Results\n",
    "            - 005 vs 008\n",
    "            - Same performance after 331 epochs\n",
    "            \n",
    "    - Test float16\n",
    "        - 005 float32\n",
    "        - 010 float16\n",
    "        - Results\n",
    "            - float16 scored lower\n",
    "            - float16 is 40% longer per epoch (35sec vs 50sec per epoch\n",
    "            - float32 is the way to go!\n",
    "            \n",
    "    - Try Batch Normalization\n",
    "        - model.add(BatchNormalization())\n",
    "        - Results:\n",
    "            - Comparing with 005, no batch norm\n",
    "            - Saved to 011, with batch norm\n",
    "            - ~same number of weights...\n",
    "            - Batch Normalization is performing worse after ~40 epochs\n",
    "            \n",
    "    - Try different initial weights\n",
    "        - keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "        - Results\n",
    "            - Tried for a few epochs, similar curve as for 005\n",
    "            \n",
    "    - Test different size model\n",
    "        - Experiments\n",
    "            - 005\n",
    "                - 50kpm\n",
    "                - 500 epochs\n",
    "                - 0.952 acc\n",
    "                - still learning\n",
    "                - 2048bs > 37s/epoch\n",
    "                - 4096bs > 29s/epoch\n",
    "            - 013\n",
    "                - 188kpm\n",
    "                - 350 epochs\n",
    "                - 0.968 acc\n",
    "                - 150-200 epochs > stalled\n",
    "                - 0.968 after 350 epochs\n",
    "                - 6 hours\n",
    "            - 014\n",
    "                - 388kpm\n",
    "                - epochs\n",
    "                - 0.974 after 243 epochs\n",
    "            - xxx not saved\n",
    "                - 500kpm\n",
    "                - 143 epochs\n",
    "                - 0.972\n",
    "                - slightly worse performance than 014, possibly slower to converge, but taking to long to train\n",
    "            - 016\n",
    "                - 10kpm\n",
    "                - 500 epochs\n",
    "                - 0.91 acc\n",
    "            - 01\n",
    "                - \n",
    "        - Results\n",
    "            - 014 is best, 005 probably good enough\n",
    "            - 017 is best for fast training, maybe?\n",
    "            \n",
    "    - Measure kpm and batch size effect on calc speed\n",
    "        - not much difference\n",
    "            - #016:0.913  10kpm 2048:30                  8192:23:38% 32768:17:52% \n",
    "            - #005:0.952  50kpm 2048:37s    4096:28:50% \n",
    "                - 0.94 after 100epoch = 3000s ~ 1 hour \n",
    "            - #013:0.968 188kpm 2048:50s    4096:40s:61%             32768:46s:80% 65536:42s:99% \n",
    "                - 0.96 after 50 epoch = 41m\n",
    "            - #014:0.974 388kpm 2048:3m:91%\n",
    "                - 0.97 after 60 epoch =180m\n",
    "            \n",
    "    - Test effect of bigger batch size\n",
    "        - Faster calculations, but much worse performance\n",
    "            - see 017 and 018\n",
    "            \n",
    "    - Try smaller batch size\n",
    "        - source 014\n",
    "            - 2048 batch size\n",
    "        - dest\n",
    "            - 256 bs\n",
    "        -Results\n",
    "            - started to forget, \n",
    "            \n",
    "     - Test if increased training after validation lággildi is doing any good\n",
    "         - 022 = 388 kpm\n",
    "         - Laggildi at 70 epochs, accuracy 0.970\n",
    "         - Trained up to 250 epochs, accuracy 0.972\n",
    "         - Results\n",
    "             - it's no use to train after the 70 epoch mark\n",
    "     \n",
    "    - count wdl histogram\n",
    "    \n",
    "    - Do transfer learning\n",
    "         - Decide whats the best net\n",
    "            - 188kpm from #013, stalled at 60 epoch 4pc. 41m for 60 epochs.\n",
    "                - 2x32-2x64-2x128-2x160-2x256\n",
    "         - Train on 3pc\n",
    "             - 024:0.998\n",
    "         - Transfer to 4pc\n",
    "             - 029: 0.964\n",
    "         - Train 4pc from scratch\n",
    "             - 028: 0.966\n",
    "         \n",
    "         \n",
    "         - How to transfer only n layers?\n",
    "         - How to freeze layers?\n",
    "         - Do n-transfer with and without freeze\n",
    "             - Use average of m runs (add average later...)\n",
    "     \n",
    "    - Find best CNN layer number\n",
    "        - trained for 250 ep\n",
    "            - 3 layers\n",
    "                - 030\n",
    "                - final 0.922 \n",
    "                - time per epoch\n",
    "            - 4 layers\n",
    "                - 032\n",
    "                - final 0.951\n",
    "                - time per epoch\n",
    "            - 5 layers\n",
    "                - 028\n",
    "                - final 0.966\n",
    "                - time per epoch\n",
    "            - 6 layers\n",
    "                - 034\n",
    "                - 16 32 64 128 128 128\n",
    "                - final 0.975\n",
    "                - time per epoch\n",
    "            - 6 layers\n",
    "                - 035\n",
    "                - 16 64 64 96 128 128\n",
    "                - final 0.975\n",
    "            - 7 layers\n",
    "                - 056\n",
    "                - 35kpm\n",
    "                - 2048 batch size\n",
    "                - final 0.956\n",
    "                - time per epoch\n",
    "            - 7 layers\n",
    "                - 054\n",
    "                - 70kpm\n",
    "                - 2048 batch size\n",
    "                - final 0.975\n",
    "                - time per epoch 40s\n",
    "                - process 65%\n",
    "            - 7 layers\n",
    "                - 050 \n",
    "                - 184kpm\n",
    "                - 2048 batch size\n",
    "                - final 0.977\n",
    "                - time per epoch 60s\n",
    "                - process 70%\n",
    "            - 7 layers\n",
    "                - 055\n",
    "                - 256kpm\n",
    "                - 2048 batch size\n",
    "                - final 0.983\n",
    "                - time per epoch 65s\n",
    "                - process 75%\n",
    "        - measure time to train on these nets, and decide on structure based on that\n",
    "            - bs 256 vs 2048\n",
    "                - accuracy 256@17 = accuracy 2048@41\n",
    "                - 256:120spb vs 2048:45spb\n",
    "                - 2048 seems to get higher final accuracy than 256, \n",
    "                \n",
    "        - test different batch size for 70kpm 7 CNN layers net\n",
    "            - Results\n",
    "                - \n",
    "             \n",
    "        \n",
    "        \n",
    "          \n",
    "                \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "    ---------- TODO NOW --------------------------------------------------     \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    - performance of 4pc seems to be negatively affected by transfer from 3pc\n",
    "        - test if I'm overfitting on 3pc, by transfering after n epochs of 3pc training\n",
    "            with n = 10, 20, 30, ...\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    ---------- TODO NEXT --------------------------------------------------\n",
    "    \n",
    "     - Test if increased training after validation lággildi is doing any good\n",
    "         - Same test as above, but with smaller net (188 kpm which is MUCH faster to train)\n",
    "             - Train both with early stopping and without, and see where it stops\n",
    "                 - keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0,\n",
    "                   patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ---------- TODO SOMETIME --------------------------------------------------\n",
    "    \n",
    "    - Do tree fold testing on 014, and focus on lággildi in loss at 50epochs\n",
    "    \n",
    "    - Transfer learning\n",
    "        - Read article again\n",
    "        - Make function for measurements\n",
    "            - Copy on n first layers\n",
    "            - freeze layers\n",
    "            - Average\n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "    - Test Checkpoints feature\n",
    "    \n",
    "    - Three fold splitting\n",
    "        - model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10)\n",
    "        - https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test\n",
    "        \n",
    "    - cross validation\n",
    "        - https://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras/\n",
    "        \n",
    "    - Train longer....\n",
    "\n",
    "    - Try opther optimizers\n",
    "\n",
    "    - Make histogram of WDL values\n",
    "    \n",
    "    - Do TL experiment\n",
    "        - Take into account data split effect on TL (split 4pc at x, and then transfer to 5pc)\n",
    "        - Three split data, training, validation and testing\n",
    "        - Results\n",
    "        \n",
    "    - Finish 5pc dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# WDL score count\n",
    "#### 3ps only legal\n",
    "WDLhistogram {-2=0, -1=0, 0=38368, 1=0, 2=124960}\n",
    "\n",
    "p = [0.0, 0.235, 0.765]\n",
    "\n",
    "#### 4pc only legal\n",
    "WDLhistogram {-2=1737970, -1=0, 0=2485090, 1=0, 2=3213028}\n",
    "\n",
    "p = [0.234, 0.334, 0.432]\n",
    "\n",
    "# Randomly guess right probabilites\n",
    "Purpose: When training net $N$ on $n$ piece dataset and using it to guess labels for a $m$ piece dataset, what is the expected accuracy of $N$ on the $m$ piece dataset? \n",
    "\n",
    "Methood: Given the approximation that net $N$ outputs labels randomly with probabilities $p_n$, and we sample from the $m$ piece dataset with label probabilites $p_m$, then the probability of guessing right is given by\n",
    "\n",
    "$$ P_{n>m} = \\sum_{i=-1}^1 p_n(i) * p_m(i)$$\n",
    "\n",
    "Results: Using calcRandomPerfProbability.py we get\n",
    "\n",
    "$$P_{3>4} = 0.409$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:endnetGpu]",
   "language": "python",
   "name": "conda-env-endnetGpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
