{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "## Done\n",
    "- finish 3n4 and 4n4\n",
    "    - 4n4 shows coapdaption in the middle \n",
    "    - 4_7_4 reaches 99% accuracy... why???\n",
    "        - **try to train many times 4_7_4 and 4_8_4, see if this is indeed a trend, is this due to the Adadelta?**\n",
    "- trained 3n4 to 300 + 300, stalled at 0.9789, with adadelta\n",
    "    - 521 and 722\n",
    "- converge rndTo4 and 3to4\n",
    "    -test 1\n",
    "        - train rndTo4 150 epochs\n",
    "            0.982\n",
    "            0.984\n",
    "            0.985\n",
    "            0.983\n",
    "            0.982\n",
    "\n",
    "            mean = 0.9832 (min 0,9819)\n",
    "            error = 0.00127776\n",
    "\n",
    "        - train 3to4 150 epochs\n",
    "            0.977\n",
    "            0.98\n",
    "            0.978\n",
    "            0.977\n",
    "            0.976\n",
    "\n",
    "            mean = 0.9776 (max 0,9791)\n",
    "            error = 0.001486\n",
    "        - rndTo4 is better, see t stuff in resultsExplained dir\n",
    "- converge 3to4 WITH RETRAINING OF 3 EVERY RUN\n",
    "    - RESULTS\n",
    "        mean = \n",
    "        error = \n",
    "    - raw data\n",
    "        - 3pc results ( 4pc are in continium of these )\n",
    "            finalAccuracy [0.996, 0.9961, 0.9958, 0.9951, 0.994]\n",
    "            finalCount [53899, 53899, 53899, 53899, 53899]\n",
    "            drawAccuracy [0.9973, 0.9976, 0.9972, 0.9966, 0.9968]\n",
    "            drawCount [41169, 41294, 41168, 41201, 41237]\n",
    "            lossCount [12730, 12605, 12731, 12698, 12662]\n",
    "            winCount [0, 0, 0, 0, 0]\n",
    "            winAccuracy [-1, -1, -1, -1, -1]\n",
    "            lossAccuracy [0.9919, 0.9914, 0.9911, 0.9901, 0.9848]\n",
    "\n",
    "        - 3pc average\n",
    "            finalAccuracy 0.9953999999999998\n",
    "            finalCount 53899.0\n",
    "\n",
    "            drawAccuracy 0.9971\n",
    "            drawCount 41213.8\n",
    "\n",
    "            lossAccuracy 0.98986\n",
    "            lossCount 12685.2\n",
    "\n",
    "            winAccuracy -1.0\n",
    "            winCount 0.0\n",
    "\n",
    "        - 4pc results ( in continium of 3pc)\n",
    "            finalAccuracy [0.9779, 0.9702, 0.9717, 0.9749, 0.9657]\n",
    "            finalCount [2453910, 2453910, 2453910, 2453910, 2453910]\n",
    "            drawAccuracy [0.9909, 0.9751, 0.9798, 0.9814, 0.9757]\n",
    "            drawCount [1058731, 1061023, 1060426, 1059689, 1060284]\n",
    "            lossAccuracy [0.9584, 0.9628, 0.9623, 0.9628, 0.9522]\n",
    "            lossCount [821028, 819381, 819155, 820525, 819884]\n",
    "            winAccuracy [0.9818, 0.9717, 0.9701, 0.9802, 0.9664]\n",
    "            winCount [574151, 573506, 574329, 573696, 573742]\n",
    "\n",
    "        - 4pc average\n",
    "            finalAccuracy 0.97208\n",
    "            finalCount 2453910.0\n",
    "\n",
    "            drawAccuracy 0.98058\n",
    "            drawCount 1060030.6 43%\n",
    "          \n",
    "            lossAccuracy 0.9597000000000001\n",
    "            lossCount 819994.6 33.4%\n",
    "\n",
    "            winAccuracy 0.9740399999999999\n",
    "            winCount 573884.8 23.4%\n",
    "           \n",
    "        \n",
    "## ToDo\n",
    "- compare adam vs adadelta\n",
    "- converge 3>4 and 4>4 a few times\n",
    "    - train a few to 150 epochs\n",
    "    - take best network and train a few to 150 epochs\n",
    "    - repeat...\n",
    "test 4_7_4 peak, by itteratively doing 4_7_4 again, and building on last network\n",
    "    \n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments - archive\n",
    "\n",
    "    - Test if small batch is better\n",
    "        - Exp 1\n",
    "            - Save to 006\n",
    "            - starting from 005, and shring batch size to 512\n",
    "            - Results: after 183 epochs, the accuracy was actually lower than before training. \n",
    "              Batch size is probably not to big at 4096.\n",
    "        - Exp 2\n",
    "            - Starting from 012, a 100.000 pm net trained for 1600 epochs at bs 4096\n",
    "            - Switching to bs 32 created horrible results, network totally failed to perform, lost acc from \n",
    "              0.95 to 0.85 in 17 epochs. Plus, super slow to train\n",
    "                 \n",
    "    - Test Adam\n",
    "        - Results\n",
    "            - 007 Adam, final accuracy 0.939 after 500\n",
    "            - 005 Adadelta, final accuracy 0.952 after 1000 epochs\n",
    "            - Adam not performing better than Adadelta\n",
    "            \n",
    "    - Test 2x2 and 3x3 filters for performance\n",
    "        - 2x2 5 layers (005)\n",
    "        - 3x3 3 layers (003)\n",
    "        - ~50.000 parameters\n",
    "        - Results\n",
    "            - 005 vs 008\n",
    "            - Same performance after 331 epochs\n",
    "            \n",
    "    - Test float16\n",
    "        - 005 float32\n",
    "        - 010 float16\n",
    "        - Results\n",
    "            - float16 scored lower\n",
    "            - float16 is 40% longer per epoch (35sec vs 50sec per epoch\n",
    "            - float32 is the way to go!\n",
    "            \n",
    "    - Try Batch Normalization\n",
    "        - model.add(BatchNormalization())\n",
    "        - Results:\n",
    "            - Comparing with 005, no batch norm\n",
    "            - Saved to 011, with batch norm\n",
    "            - ~same number of weights...\n",
    "            - Batch Normalization is performing worse after ~40 epochs\n",
    "            \n",
    "    - Try different initial weights\n",
    "        - keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "        - Results\n",
    "            - Tried for a few epochs, similar curve as for 005\n",
    "            \n",
    "    - Test different size model\n",
    "        - Experiments\n",
    "            - 005\n",
    "                - 50kpm\n",
    "                - 500 epochs\n",
    "                - 0.952 acc\n",
    "                - still learning\n",
    "                - 2048bs > 37s/epoch\n",
    "                - 4096bs > 29s/epoch\n",
    "            - 013\n",
    "                - 188kpm\n",
    "                - 350 epochs\n",
    "                - 0.968 acc\n",
    "                - 150-200 epochs > stalled\n",
    "                - 0.968 after 350 epochs\n",
    "                - 6 hours\n",
    "            - 014\n",
    "                - 388kpm\n",
    "                - epochs\n",
    "                - 0.974 after 243 epochs\n",
    "            - xxx not saved\n",
    "                - 500kpm\n",
    "                - 143 epochs\n",
    "                - 0.972\n",
    "                - slightly worse performance than 014, possibly slower to converge, but taking to long to train\n",
    "            - 016\n",
    "                - 10kpm\n",
    "                - 500 epochs\n",
    "                - 0.91 acc\n",
    "            - 01\n",
    "                - \n",
    "        - Results\n",
    "            - 014 is best, 005 probably good enough\n",
    "            - 017 is best for fast training, maybe?\n",
    "            \n",
    "    - Measure kpm and batch size effect on calc speed\n",
    "        - not much difference\n",
    "            - #016:0.913  10kpm 2048:30                  8192:23:38% 32768:17:52% \n",
    "            - #005:0.952  50kpm 2048:37s    4096:28:50% \n",
    "                - 0.94 after 100epoch = 3000s ~ 1 hour \n",
    "            - #013:0.968 188kpm 2048:50s    4096:40s:61%             32768:46s:80% 65536:42s:99% \n",
    "                - 0.96 after 50 epoch = 41m\n",
    "            - #014:0.974 388kpm 2048:3m:91%\n",
    "                - 0.97 after 60 epoch =180m\n",
    "            \n",
    "    - Test effect of bigger batch size\n",
    "        - Faster calculations, but much worse performance\n",
    "            - see 017 and 018\n",
    "            \n",
    "    - Try smaller batch size\n",
    "        - source 014\n",
    "            - 2048 batch size\n",
    "        - dest\n",
    "            - 256 bs\n",
    "        -Results\n",
    "            - started to forget, \n",
    "            \n",
    "     - Test if increased training after validation lággildi is doing any good\n",
    "         - 022 = 388 kpm\n",
    "         - Laggildi at 70 epochs, accuracy 0.970\n",
    "         - Trained up to 250 epochs, accuracy 0.972\n",
    "         - Results\n",
    "             - it's no use to train after the 70 epoch mark\n",
    "     \n",
    "    - count wdl histogram\n",
    "    \n",
    "    - Do transfer learning\n",
    "         - Decide whats the best net\n",
    "            - 188kpm from #013, stalled at 60 epoch 4pc. 41m for 60 epochs.\n",
    "                - 2x32-2x64-2x128-2x160-2x256\n",
    "         - Train on 3pc\n",
    "             - 024:0.998\n",
    "         - Transfer to 4pc\n",
    "             - 029: 0.964\n",
    "         - Train 4pc from scratch\n",
    "             - 028: 0.966\n",
    "         \n",
    "         \n",
    "         - How to transfer only n layers?\n",
    "         - How to freeze layers?\n",
    "         - Do n-transfer with and without freeze\n",
    "             - Use average of m runs (add average later...)\n",
    "     \n",
    "    - Find best CNN layer number\n",
    "        - trained for 250 ep\n",
    "            - 3 layers\n",
    "                - 030\n",
    "                - final 0.922 \n",
    "                - time per epoch\n",
    "            - 4 layers\n",
    "                - 032\n",
    "                - final 0.951\n",
    "                - time per epoch\n",
    "            - 5 layers\n",
    "                - 028\n",
    "                - final 0.966\n",
    "                - time per epoch\n",
    "            - 6 layers\n",
    "                - 034\n",
    "                - 16 32 64 128 128 128\n",
    "                - final 0.975\n",
    "                - time per epoch\n",
    "            - 6 layers\n",
    "                - 035\n",
    "                - 16 64 64 96 128 128\n",
    "                - final 0.975\n",
    "            - 7 layers\n",
    "                - 056\n",
    "                - 35kpm\n",
    "                - 2048 batch size\n",
    "                - final 0.956\n",
    "                - time per epoch\n",
    "            - 7 layers\n",
    "                - 054\n",
    "                - 70kpm\n",
    "                - 2048 batch size\n",
    "                - final 0.975\n",
    "                - time per epoch 40s\n",
    "                - process 65%\n",
    "            - 7 layers\n",
    "                - 050 \n",
    "                - 184kpm\n",
    "                - 2048 batch size\n",
    "                - final 0.977\n",
    "                - time per epoch 60\n",
    "                - process 70%\n",
    "            - 7 layers\n",
    "                - 055\n",
    "                - 256kpm\n",
    "                - 2048 batch size\n",
    "                - final 0.983\n",
    "                - time per epoch 65s\n",
    "                - process 75%\n",
    "        - measure time to train on these nets, and decide on structure based on that\n",
    "            - bs 256 vs 2048\n",
    "                - accuracy 256@17 = accuracy 2048@41\n",
    "                - 256:120spb vs 2048:45spb\n",
    "                - 2048 seems to get higher final accuracy than 256, \n",
    "                \n",
    "        - test different batch size for 70kpm 7 CNN layers net\n",
    "            - Results\n",
    "                - 079  256 0.973\n",
    "                - 080  512 0.97\n",
    "                - 081 1024 0.973\n",
    "                - 082 2048 0.963\n",
    "                - 083 4096 0.967\n",
    "                - 084 8194 0.937\n",
    "             \n",
    "        \n",
    "        \n",
    "          \n",
    "                \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "    ---------- TODO NOW --------------------------------------------------     \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    - performance of 4pc seems to be negatively affected by transfer from 3pc\n",
    "        - test if I'm overfitting on 3pc, by transfering after n epochs of 3pc training\n",
    "            with n = 10, 20, 30, ...\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    ---------- TODO NEXT --------------------------------------------------\n",
    "    \n",
    "     - Test if increased training after validation lággildi is doing any good\n",
    "         - Same test as above, but with smaller net (188 kpm which is MUCH faster to train)\n",
    "             - Train both with early stopping and without, and see where it stops\n",
    "                 - keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0,\n",
    "                   patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ---------- TODO SOMETIME --------------------------------------------------\n",
    "    \n",
    "    - Do tree fold testing on 014, and focus on lággildi in loss at 50epochs\n",
    "    \n",
    "    - Transfer learning\n",
    "        - Read article again\n",
    "        - Make function for measurements\n",
    "            - Copy on n first layers\n",
    "            - freeze layers\n",
    "            - Average\n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "    - Test Checkpoints feature\n",
    "    \n",
    "    - Three fold splitting\n",
    "        - model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10)\n",
    "        - https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test\n",
    "        \n",
    "    - cross validation\n",
    "        - https://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras/\n",
    "        \n",
    "    - Train longer....\n",
    "\n",
    "    - Try opther optimizers\n",
    "\n",
    "    - Make histogram of WDL values\n",
    "    \n",
    "    - Do TL experiment\n",
    "        - Take into account data split effect on TL (split 4pc at x, and then transfer to 5pc)\n",
    "        - Three split data, training, validation and testing\n",
    "        - Results\n",
    "        \n",
    "    - Finish 5pc dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WDL score count\n",
    "#### 3ps only legal\n",
    "WDLhistogram {-2=0, -1=0, 0=38368, 1=0, 2=124960}\n",
    "\n",
    "p = [0.0, 0.235, 0.765]\n",
    "\n",
    "#### 4pc only legal\n",
    "WDLhistogram {-2=1737970, -1=0, 0=2485090, 1=0, 2=3213028}\n",
    "\n",
    "p = [0.234, 0.334, 0.432]\n",
    "\n",
    "#### 5pc only legal\n",
    "WDLhistogram {-2: 20565590, -1: 0, 0: 16700358, 1: 3668, 2: 123665832}\n",
    "-2 : 20565590\n",
    "-1 : 0\n",
    "0 : 16700358\n",
    "1 : 3668\n",
    "2 : 123665832\n",
    "\n",
    "p = [0.128, 0.000, 0.104, 0.00002, 0.768]\n",
    "\n",
    "p = [0.128,  0.104, 0.768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 38368 124960\n",
      "1737970 2485090 3213028\n",
      "20565590 16700358 123669500\n"
     ]
    }
   ],
   "source": [
    "x3 = {-2:0, -1:0, 0:38368, 1:0, 2:124960}\n",
    "\n",
    "x4 =  {-2:1737970, -1:0, 0:2485090, 1:0, 2:3213028}\n",
    "\n",
    "x5 =  {-2: 20565590, -1: 0, 0: 16700358, 1: 3668, 2: 123665832}\n",
    "\n",
    "x = x3\n",
    "print(x[-2] + x[-1],x[0], x[2] + x[1] )\n",
    "x = x4\n",
    "print(x[-2] + x[-1],x[0], x[2] + x[1] )\n",
    "x = x5\n",
    "print(x[-2] + x[-1],x[0], x[2] + x[1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of states\n",
    "## 4 pc\n",
    "Only pawn states = 7,436,088\n",
    "All piece states = 125,246,598\t\n",
    "## 5 pc\n",
    "Only pawn states = 160,935,448\n",
    "All piece states = 25,912,594,054 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4pc 7436088\n",
      "x4full/x4 16\n",
      "\n",
      "5pc 160935448\n",
      "x5full/x5 161\n",
      "\n",
      "x5/x4 21\n",
      "x5full/x4full 206\n",
      "x7full/x4full 3384018\n",
      "26026\n"
     ]
    }
   ],
   "source": [
    "x5 =20565590 + 16700358 + 3668 + 123665832\n",
    "x4 =1737970 + 2485090 + 3213028\n",
    "x7full = 423836835667331\n",
    "x4full = 125246590\n",
    "x5full = 25912594054\n",
    "print('4pc',x4)\n",
    "print('x4full/x4', x4full//x4)\n",
    "print()\n",
    "print('5pc',x5)\n",
    "print('x5full/x5', x5full//x5)\n",
    "print()\n",
    "print('x5/x4',x5//x4)\n",
    "print('x5full/x4full',x5full//x4full)\n",
    "print('x7full/x4full',x7full//x4full)\n",
    "\n",
    "trainTime4Pawns = 4\n",
    "traininTime7 = x7full//x4full * x4full//x4 * trainTime4Pawns\n",
    "print(traininTime7//24//365)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomly guess right probabilites\n",
    "\n",
    "Purpose: When training net $N$ on $n$ piece dataset and using it to guess labels for a $m$ piece dataset, what is the expected accuracy of $N$ on the $m$ piece dataset? \n",
    "\n",
    "Methood: Given the approximation that net $N$ outputs labels randomly with probabilities $p_n$, and we sample from the $m$ piece dataset with label probabilites $p_m$, then the probability of guessing right is given by\n",
    "\n",
    "$$ P_{n\\rightarrow m} =  \\sum_{i=-1}^1 p_n(i) * p_m(i)$$\n",
    "\n",
    "Expected random sampling probability (calcRandomPerfProbability.py) for n = 3 and m = 4 we have $P_{3\\rightarrow 4} = 0.409$\n",
    "\n",
    "THIS ABOVE IS PROBABLY NO USABLE!!!!\n",
    "\n",
    "### !!!!!!!!!!! DO THIS AGAIN WITH THE WHOLE DATASET... !!!!!!!!!!!!!!!!!!\n",
    "\n",
    "#### What is the expected guess-right probability given random guessing of labels\n",
    "\n",
    "**Problem:**\n",
    "Given a random distribution A and random variable X~A, X$\\in${1,2,3}, with f.x. probabilities $P_{\\textrm{A}}(X = 1) = P_{\\textrm{A}}(X = 2) = 0.1$ and $P_{\\textrm{A}}(X = 3) = 0.8$. Let's say I'm sampling from this distribution, and also throwing a fair (non-biased) 3-sided dice with numbers 1, 2 and 3. What is the probability of the number on the dice and the sampling from the distribution agree?\n",
    "\n",
    "**My solution**\n",
    "From intuition I would say that the probability is:\n",
    "\n",
    "$P = \\sum_{i=1}^3 P_{\\textrm{Dice}}(i) * P_{\\textrm{A}}(X = i)$\n",
    "\n",
    "**The problem**\n",
    "\n",
    "This gives us $P =  \\sum_{i=1}^3 \\frac{1}{3} * P_{A}(X = i) = \\frac{1}{3} \\sum_{i=1}^3  P_{A}(X = i) = \\frac{1}{3} 1 = \\frac{1}{3}$\n",
    "\n",
    "So the distribution we are sampling from doesn't matter..., the answer is always 1/3.  I guess that might be ok, but I'm not so sure. Am I on the right track? :)\n",
    "\n",
    "#### Results:\n",
    "**Random guessing would give us 1/3. The random net gives us 0.266 and the 3-piece net gives us 0.571. So the untrained net is worse than random, and the 3-piece net is 71\\% better than random. Not great but usable... to some degree.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expansion\n",
    "\n",
    "### $ 3 \\rightarrow 4$\n",
    "\n",
    "RND results: $P_{RND \\rightarrow 4} = 0.266$\n",
    "\n",
    "Expanded results: $P_{3\\rightarrow 4} = 0.571$\n",
    "\n",
    "### $ 4 \\rightarrow 5$\n",
    "\n",
    "RND results: $P_{RND \\rightarrow 5} = 0.183$\n",
    "\n",
    "Expanded results: $P_{4\\rightarrow 5} = 0.523$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training times\n",
    "\n",
    "\n",
    "|ds|kpm|epochs | time| tpe| t/150ep|\n",
    "|--|--|--|--|--|--|\n",
    "|4pc|70|100| 4:42 | 2.82 | 7:20|\n",
    "|4pc|181|150|7:11|2.9|7:11|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results from transfer learning \n",
    "## $3\\rightarrow 4$\n",
    "Averaged over 5 trainings, 150 epochs, results 106-115, 5 outputs\n",
    "\n",
    "Acc$(\\phi_{rnd}(D_4)) = 0.978 \\pm 0.003$\n",
    "\n",
    "Acc$(\\phi_{3}(D_4)) = 0.9750 \\pm 0.0003$ \n",
    "\n",
    "### Results\n",
    "From https://www.socscistatistics.com/tests/studentttest/Default2.aspx: The t-value is -1.31848. The p-value is .223839. The result is not significant at p < .01.\n",
    "\n",
    "Transfer from 3 doesn't make any difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$3 \\rightarrow 4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
