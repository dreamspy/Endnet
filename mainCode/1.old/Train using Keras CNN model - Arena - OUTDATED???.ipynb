{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiGPU = False\n",
    "whichGPU = 0\n",
    " \n",
    "# Select which GPU to use\n",
    "if(multiGPU):\n",
    "    from keras.utils.training_utils import multi_gpu_model\n",
    "else:\n",
    "    import os\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "    # The GPU id to use, usually either \"0\" or \"1\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(whichGPU)\n",
    "    \n",
    "# # Do other imports now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i 'Arena.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "#\n",
    "#    IMPORTS \n",
    "#\n",
    "##############################\n",
    "\n",
    "import os\n",
    "    \n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# from __future__ import print_function\n",
    "import keras\n",
    "# from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Activation\n",
    "from keras import backend as K\n",
    "# import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "#Tensorboard\n",
    "# from time import time\n",
    "import time\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "\n",
    "# Weight Checkpoints\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Move directories\n",
    "import shutil\n",
    "\n",
    "# debuging\n",
    "import ipdb\n",
    "# ipdb.set_trace()\n",
    "\n",
    "# Print progress\n",
    "from decimal import Decimal\n",
    "\n",
    "##############################\n",
    "#\n",
    "#    Plot Losses Callback\n",
    "#\n",
    "##############################\n",
    "\n",
    "class PlotLosses(keras.callbacks.Callback):\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        # Reshape input vector to fit on graph\n",
    "        def reshapeVector(vec):\n",
    "            l = len(vec)\n",
    "            L = epochs - l\n",
    "            if L>=0:\n",
    "                tail = np.ones((L), dtype = int) * vec[-1]\n",
    "                vec = np.hstack((vec,tail))\n",
    "            return vec\n",
    "                \n",
    "        \n",
    "        # Load data to compare with \n",
    "        if compareResultsDuringTraining:\n",
    "            self.compareData = load_obj('Results/' + compareWith, 'fitHistory')\n",
    "            self.compAcc = reshapeVector(self.compareData['acc'])\n",
    "            self.compValAcc = reshapeVector(self.compareData['val_acc'])\n",
    "            self.compLoss = reshapeVector(self.compareData['loss'])\n",
    "            self.compValLoss = reshapeVector(self.compareData['val_loss'])\n",
    "        \n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        self.acc = []\n",
    "        self.val_acc = []\n",
    "        self.fig = plt.figure()\n",
    "        self.logs = {'acc':[], 'val_acc':[], 'loss':[], 'val_loss':[]}\n",
    "        self.saveDir = 'Results/' + str(resID) + '/fitTemp/'\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.x.append(self.i)\n",
    "        self.loss.append(logs['loss'])\n",
    "        self.val_loss.append(logs['val_loss'])\n",
    "        self.acc.append(logs['acc'])\n",
    "        self.val_acc.append(logs['val_acc'])\n",
    "        self.logs = {'acc':self.acc, 'val_acc':self.val_acc, 'loss':self.loss, 'val_loss':self.val_loss}\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "    \n",
    "        # Create plots\n",
    "        f = plt.figure(figsize=(15,7))\n",
    "        ax = f.add_subplot(121)\n",
    "        ax2 = f.add_subplot(122)\n",
    "        \n",
    "        \n",
    "        # Plot Loss \n",
    "        ax.plot(self.x, self.loss, color='blue', label=\"Train\", linewidth = 1)\n",
    "        ax.plot(self.x, self.val_loss, color='deepskyblue', label=\"Validation\", linewidth = 1)\n",
    "        if compareResultsDuringTraining:\n",
    "            ax.plot(self.x, self.compLoss[:len(self.loss)], color='black', label=compareWith + \" Training\", linewidth = 1)\n",
    "            ax.plot(self.x, self.compValLoss[:len(self.loss)], color='gray', label=compareWith + \" Validation\", linewidth = 1)\n",
    "        ax.set_xlabel('Epochs')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.legend()\n",
    "        ax.set_ylim(bottom=0)\n",
    "        ax.grid(True)\n",
    "        \n",
    "#         # Plot Accuracy\n",
    "        ax2.plot(self.x, self.acc, 'b-', label=\"Train\", linewidth = 1)\n",
    "        ax2.plot(self.x, self.val_acc, color = 'deepskyblue', label=\"Validation\", linewidth = 1)\n",
    "        if compareResultsDuringTraining:\n",
    "            ax2.plot(self.x, self.compAcc[:len(self.acc)], color='black', label=compareWith + \" Training\", linewidth = 1)\n",
    "            ax2.plot(self.x, self.compValAcc[:len(self.acc)], color='silver', label=compareWith + \" Validation\", linewidth = 1)\n",
    "        ax.set\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Accuracty')\n",
    "        ax2.legend()\n",
    "        ax2.set_ylim(top=1)\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        # Show and save plot\n",
    "# #         plt.tight_layout()\n",
    "        plt.savefig(self.saveDir + 'currentAccAndLoss')\n",
    "        plt.show();\n",
    "\n",
    "#         # Plot Loss\n",
    "#         plt.subplot(1,2,1)\n",
    "#         plt.figure(figsize=(8,8))\n",
    "#         plt.plot(self.x, self.loss, 'b-', label=\"Train\", linewidth = 1)\n",
    "#         plt.plot(self.x, self.val_loss, 'r-', label=\"Validation\", linewidth = 1)\n",
    "#         plt.plot(self.x, self.compLoss[:len(self.loss)], 'b--', label=compareWith + \" Training\")\n",
    "#         plt.plot(self.x, self.compValLoss[:len(self.loss)], 'r--', label=compareWith + \" Validation\")\n",
    "#         plt.xlabel('Epochs')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.legend()\n",
    "#         plt.ylim(bottom=0)\n",
    "#         plt.grid(True)\n",
    "# #         plt.savefig('fitTemp/currentLoss')\n",
    "# #         plt.show();\n",
    "        \n",
    "#         # Plot Accuracy\n",
    "#         plt.subplot(1,2,2)\n",
    "#         plt.figure(figsize=(8,8))\n",
    "#         plt.plot(self.x, self.acc, 'b-', label=\"Train\", linewidth = 1)\n",
    "#         plt.plot(self.x, self.val_acc, 'r-', label=\"Validation\", linewidth = 1)\n",
    "#         plt.plot(self.x, self.compAcc[:len(self.acc)], 'b--', label=compareWith + \" Training\")\n",
    "#         plt.plot(self.x, self.compValAcc[:len(self.acc)], 'r--', label=compareWith + \" Validation\")\n",
    "#         plt.xlabel('Epochs')\n",
    "#         plt.ylabel('Accuracty')\n",
    "#         plt.legend()\n",
    "#         plt.ylim(top=1)\n",
    "#         plt.grid(True)\n",
    "        \n",
    "#         # Show and save plot\n",
    "# #         plt.tight_layout()\n",
    "# #         plt.savefig('fitTemp/currentAccAndLoss')\n",
    "#         plt.show();\n",
    "        \n",
    "        print(\"Train Accuracy of last epoch: \", logs['acc'])\n",
    "        print(\"Validation Accuracy of last epoch: \", logs['val_acc'])\n",
    "        print(\"Train Loss of last epoch: \", logs['loss'])\n",
    "        print(\"Validation Loss of last epoch: \", logs['val_loss'])\n",
    "        \n",
    "        with open(self.saveDir + 'logs.txt','w') as file:\n",
    "            file.write(str(self.logs))\n",
    "            \n",
    "        with open(self.saveDir + 'atEpochNr.txt','w') as file:\n",
    "            file.write(str(epoch))\n",
    "        \n",
    "plot_losses = PlotLosses()\n",
    "\n",
    "##############################\n",
    "#\n",
    "#    Misc Functions\n",
    "#\n",
    "##############################\n",
    " \n",
    "def calcScore(model):\n",
    "    print(\"Calculating score\")\n",
    "    score = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(X_train.shape)    \n",
    "    print('Evaluated test loss:', score[0])\n",
    "    print('Evaluated test accuracy:', score[1])\n",
    "    return score\n",
    "\n",
    "def calcScoreBigData(model):\n",
    "    print(\"Calculating score\")\n",
    "    score = np.array([.0,.0])\n",
    "    t0 = time.time() # start time\n",
    "    t1 = t0 # last print\n",
    "    i1 = i2 = 0\n",
    "    \n",
    "    for X_train, X_test, y_train, y_test, percDone, loadLength in genData(test_size = 0, yieldSize = yieldSize):\n",
    "        score += np.array(model.evaluate(X_train, y_train, verbose=0))\n",
    "        t2 = time.time()\n",
    "        tSinceLastPrint = t2 - t1\n",
    "        i2 += 1    \n",
    "        if tSinceLastPrint > tPrintInterval:\n",
    "            printProgress(t0, t1, t2, i1, i2, loadLength//yieldSize)\n",
    "            t1 = time.time()\n",
    "            i1 = i2\n",
    "    score = score/(i2 + 1) \n",
    "    print()\n",
    "    print('Evaluated loss:', score[0])\n",
    "    print('Evaluated accuracy:', score[1])\n",
    "    return score \n",
    "\n",
    "def copyDirectory(src, dest):\n",
    "    try:\n",
    "        shutil.copytree(src, dest)\n",
    "    # Directories are the same\n",
    "    except shutil.Error as e:\n",
    "        print('Directory not copied. Error: %s' % e)\n",
    "    # Any error saying that the directory doesn't exist\n",
    "    except OSError as e:\n",
    "        print('Directory not copied. Error: %s' % e)\n",
    "        \n",
    "def askAbortIfPathExists(fileName): \n",
    "    if askForConfirmation:\n",
    "        if os.path.exists(fileName):\n",
    "            a = input(\"Error, file/directory {} exists, continue? [y/n]\".format(fileName))\n",
    "            if a[0] != \"y\" and a[0] != \"Y\":\n",
    "                sys.exit()\n",
    "            \n",
    "def createDir(dir, confirm = True):\n",
    "    if os.path.exists(dir):\n",
    "        askAbortIfPathExists(dir) \n",
    "    else:\n",
    "        os.makedirs(dir)\n",
    "        \n",
    "def save_obj(saveDir, saveName, obj ):\n",
    "    if not os.path.exists(saveDir):\n",
    "        os.makedirs(saveDir)\n",
    "    fileName = saveDir + '/'+ saveName + '.pkl'\n",
    "    askAbortIfPathExists(fileName)\n",
    "    with open(fileName, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(dir, fileName ):\n",
    "    with open(dir + '/' + fileName + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def sq2hnit(sq):\n",
    "    col = sq%8\n",
    "    row = (sq - col)//8\n",
    "    return col,row\n",
    "\n",
    "# 0: pawns\n",
    "# 1: kings\n",
    "def vecSt2fullSt(vecSt, nPi, nPa, nWPa):\n",
    "    fullSt = np.zeros((4,8,8), dtype = 'bool')\n",
    "    for i in range(nPi - 2):\n",
    "        sq = vecSt[i]\n",
    "        col,row = sq2hnit(sq)\n",
    "        if i < nWPa:\n",
    "            fullSt[0][row][col] = True\n",
    "        else:\n",
    "            fullSt[1][row][col] = True\n",
    "    col,row = sq2hnit(vecSt[-2])\n",
    "    fullSt[2][row][col] = True\n",
    "    col,row = sq2hnit(vecSt[-1])\n",
    "    fullSt[3][row][col] = True\n",
    "    return fullSt \n",
    "\n",
    "def vecSt2fullSt_8x8x2(vecSt, nPi, nPa, nWPa):\n",
    "    fullSt = np.zeros((8,8,2), dtype = 'int8')\n",
    "    for i in range(nPi - 2):\n",
    "        sq = vecSt[i]\n",
    "        col,row = sq2hnit(sq)\n",
    "        if i < nWPa:\n",
    "            fullSt[row][col][0] = 1\n",
    "        else:\n",
    "            fullSt[row][col][0] = -1\n",
    "    col,row = sq2hnit(vecSt[-2])\n",
    "    fullSt[row][col][1] = 1\n",
    "    col,row = sq2hnit(vecSt[-1])\n",
    "    fullSt[row][col][1] = -1\n",
    "    return fullSt \n",
    "\n",
    "# count nr of each score instance\n",
    "# wdlCounter placeholders: [-2, -1, 0, 1 ,2]\n",
    "\n",
    "def wdlCountingMachine(ds):\n",
    "    wdlCounter = [0,0,0,0,0]\n",
    "    l = len(ds)\n",
    "    i = 0\n",
    "    intv = l//100\n",
    "    for wdl in ds:\n",
    "        i += 1\n",
    "        if i%intv == 0:\n",
    "            sys.stdout.write(str((i*100)//l) + \" percentage\")\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.flush()\n",
    "        wdlCounter[wdl[0] + 2] += 1\n",
    "    print(wdlCounter)\n",
    "    return wdlCounter\n",
    "# wdlCountingMachine(d3t)\n",
    "\n",
    "##############################\n",
    "#\n",
    "#    Gen DATA\n",
    "#\n",
    "##############################\n",
    "def genData(randomState = 42, test_size = 0.33, yieldSize = 1000):\n",
    "    with h5py.File(fileName, 'r') as f:\n",
    "        d = f[dataSetName]\n",
    "        dt = f[dataSetWdlName]\n",
    "        l = len(d)\n",
    "        \n",
    "        loadLength = int(l * fractionOfDataToUse)\n",
    "\n",
    "        if convertStates:\n",
    "            sys.exit(\"loadDataGenerator can't convert states, aborting.\")\n",
    "            \n",
    "        for i in range(0,loadLength, yieldSize):\n",
    "            if i + yieldSize > loadLength:\n",
    "                ys = loadLength - i\n",
    "            else:\n",
    "                ys = yieldSize\n",
    "            X = d[i: i+ys]\n",
    "            y = dt[i: i+ys]\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=randomState)\n",
    "            del X, y\n",
    "            \n",
    "            # convert class vectors to binary class matrices\n",
    "            y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "            y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "            \n",
    "            # Percentage done\n",
    "            percDone = round(100*i/loadLength, 3)\n",
    "            \n",
    "            yield X_train, X_test, y_train, y_test, percDone, loadLength\n",
    "            \n",
    "            \n",
    "##############################\n",
    "#\n",
    "#    LOAD DATA\n",
    "#\n",
    "##############################\n",
    "# load datasets\n",
    "def loadData(randomState = 42, test_size = 0.33):\n",
    "    with h5py.File(fileName, 'r') as f:\n",
    "        d = f[dataSetName]\n",
    "        dt = f[dataSetWdlName]\n",
    "        l = len(d)\n",
    "        loadLength = int(l * fractionOfDataToUse)\n",
    "\n",
    "        if convertStates:\n",
    "            X = np.array([vecSt2fullSt(vecSt,nPi, nPa, nWPa) for vecSt in d[:loadLength]])\n",
    "        else:\n",
    "            X = d[:loadLength]\n",
    "        y = dt[:loadLength]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=randomState)\n",
    "\n",
    "    del X\n",
    "    del y\n",
    "\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print('y_train shape:', y_train.shape)\n",
    "    print('X_test shape:', X_test.shape)\n",
    "    print('y_test shape:', y_test.shape)\n",
    "    print(X_train.shape[0], 'train samples')\n",
    "    print(X_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    print(\"Done loading dataset\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "##############################\n",
    "#\n",
    "#    CREATE MODEL\n",
    "#\n",
    "##############################\n",
    "def createModel():\n",
    "    # import keras.backend as K\n",
    "    # K.set_floatx('float16')\n",
    "    # K.set_epsilon(1e-4) #default is 1e-7\n",
    "    # K.set_floatx('float32')\n",
    "    # K.set_epsilon(1e-7) #default is 1e-7\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    nnStr = ''\n",
    "    for i in range(len(filters)):\n",
    "        s = str(filterShape[i])\n",
    "        filter = str(filters[i])\n",
    "        nnStr += s + 'x' + filter + '-'\n",
    "    nnStr = nnStr[:-1]\n",
    "\n",
    "    assert (len(filters) == len(filterShape)),\"Error, len(filters) != len(filterShape)\"\n",
    "    if useBatchNorm:\n",
    "        for i in range(len(filters)):\n",
    "            if i  == 0:\n",
    "                model.add(Conv2D(filters[i], kernel_size=(filterShape[i], filterShape[i]),\n",
    "                                 padding='valid',\n",
    "                                 data_format = \"channels_first\",\n",
    "                                 use_bias = False,\n",
    "                #                  kernel_initializer = \n",
    "                                 input_shape=input_shape))\n",
    "            else:\n",
    "                model.add(Conv2D(filters[i], kernel_size=(filterShape[i], filterShape[i]),\n",
    "                                 use_bias = False,\n",
    "                                 padding='valid'))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Activation(\"relu\"))\n",
    "    else:\n",
    "        for i in range(len(filters)):\n",
    "            if i  == 0:\n",
    "                model.add(Conv2D(filters[i], kernel_size=(filterShape[i], filterShape[i]),\n",
    "                                 padding='valid',\n",
    "                                 activation='relu',\n",
    "                                 data_format = \"channels_first\",\n",
    "    #                              kernel_initializer = keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None),\n",
    "                                 input_shape=input_shape))\n",
    "            else:\n",
    "                model.add(Conv2D(filters[i], kernel_size=(filterShape[i], filterShape[i]),\n",
    "                                 padding='valid',\n",
    "    #                              kernel_initializer = keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None),\n",
    "                                 activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    if multiGPU: \n",
    "        model = keras.utils.multi_gpu_model(model, gpus=2)\n",
    "    model.summary()\n",
    "    \n",
    "    if loadWeights:\n",
    "        if loadCheckpointWeights:\n",
    "            if weightsCheckpoint[:-5] == '.hdf5':\n",
    "                weightsPath = 'Results/' + weightsSource + '/weightsCheckpoints/' + weightsCheckpoint        \n",
    "            else:\n",
    "                weightsPath = 'Results/' + weightsSource + '/weightsCheckpoints/' + weightsCheckpoint + '.hdf5'        \n",
    "        else:\n",
    "            weightsPath = 'Results/' + weightsSource + '/weights.hdf5'\n",
    "        \n",
    "        print(\"Loading weights from {}\".format(weightsPath))\n",
    "        model.load_weights(weightsPath)\n",
    "    else:\n",
    "        print(\"Starting with random weights\")\n",
    "\n",
    "    if optimizer == \"Adadelta\":\n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                      optimizer=keras.optimizers.Adadelta(),\n",
    "                      metrics=['accuracy'])\n",
    "    elif optimizer == 'Adam':\n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                      optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False),\n",
    "                      metrics=['accuracy'])\n",
    "    else:\n",
    "        sys.exit(\"Error, invalid optimizer.\")\n",
    "        \n",
    "    print(\"Done creating model\")\n",
    "    return model, nnStr\n",
    "\n",
    "##############################\n",
    "#\n",
    "#    TRAIN MODEL\n",
    "#\n",
    "##############################\n",
    "def trainModel(resID, model):\n",
    "    # save weight checkpoint\n",
    "    saveWeigthsPath = \"Results/\" + resID + '/weightsCheckpoints/'\n",
    "    print(\"Saving weights to {}\".format(saveWeigthsPath))\n",
    "    createDir(saveWeigthsPath)\n",
    "    filepath = saveWeigthsPath + \"weights-checkp-{epoch:03d}-{val_acc:.3f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    \n",
    "    kpm = model.count_params()//1000\n",
    "    dateTime = time.strftime('%Y-%m-%d-%H:%M:%S', time.localtime())\n",
    "    \n",
    "    if loadWeights:\n",
    "        initWeightsId = weightsSource\n",
    "    else:\n",
    "        initWeightsId = 'RND'\n",
    "        \n",
    "    logDir = './logs/{}-{}pc-{}-{}KPM-{}'.format(resID,nPi, initWeightsId, kpm, expDescr, dateTime )\n",
    "    \n",
    "    fitHistory = model.fit(X_train, y_train,\n",
    "                           batch_size=batch_size,\n",
    "                           epochs=epochs,\n",
    "                           verbose=1,\n",
    "                           callbacks=[checkpoint, \n",
    "                                      plot_losses, \n",
    "                                      keras.callbacks.TensorBoard(log_dir=logDir)],\n",
    "#                          .format(resID,nPi, initWeightsId, kpm, int(time() - 1500000000)))],\n",
    "                           validation_data=(X_test, y_test))\n",
    "    print(\"Training done\")\n",
    "    return fitHistory, logDir\n",
    "\n",
    "##############################\n",
    "#\n",
    "#    SAVE RESULTS\n",
    "#\n",
    "##############################\n",
    "def genNextResultsDir(model, resID = None):\n",
    "    if resID == None:\n",
    "        #Get next resID\n",
    "        with open('Results/lastResId.txt','r') as file:\n",
    "            lastId = file.read()\n",
    "        resID = str(int(lastId) + 1).zfill(3)\n",
    "\n",
    "        #Iterate resID \n",
    "        with open('Results/lastResId.txt','w') as file:\n",
    "            file.write(resID)\n",
    "    \n",
    "    # Generate save dir\n",
    "    saveDir = 'Results/' + str(resID) + '/'\n",
    "    print('Save dir: ' + saveDir)\n",
    "    print(\"Creating save dir\")\n",
    "    createDir(saveDir, confirm = True)\n",
    "\n",
    "    # Save info directories\n",
    "    if loadWeights:\n",
    "        initWeightsId = weightsSource\n",
    "    else:\n",
    "        initWeightsId = 'RND'\n",
    "        \n",
    "    kpm = str(model.count_params()//1000) + 'kpm'\n",
    "        \n",
    "    createDir(saveDir + '_' +  '_0.experimentDesc-------' + str(expDescr)) \n",
    "    createDir(saveDir + '_' +  '_1.numberOfPieces-------' + str(nPi)) \n",
    "    createDir(saveDir + '_' +  '_2.neuralNetStructure---' + str(nnStr))\n",
    "    createDir(saveDir + '_' +  '_3.loadedWeightsFrom----' +  str(initWeightsId))\n",
    "    createDir(saveDir + '_' +  '_5.batchSize------------' +  str(batch_size))\n",
    "    createDir(saveDir + '_' +  '_6.optimizer------------' +  str(optimizer))\n",
    "    createDir(saveDir + '_' +  '_7.nrOfparameters-------' +  str(kpm))\n",
    "    createDir(saveDir + '_' +  '_9.multiGPU-------------' +  str(multiGPU))\n",
    "    createDir(saveDir + 'fitTemp')\n",
    "    \n",
    "    with open(saveDir + 'fitTemp/startTime.txt', 'w') as file:\n",
    "        file.write(str(time.time()))\n",
    "        \n",
    "    print(\"Done generating results dir {}\".format(saveDir))\n",
    "    return resID\n",
    "\n",
    "def saveTrainResults(resID, model, logDir):\n",
    "    print(\"Saving results to dir {}\".format(resID))\n",
    "    saveDir = 'Results/' + str(resID) + '/'\n",
    "    ep = len(model.history.history['acc'])\n",
    "    createDir(saveDir + '_' +  '_4.epochs---------------' +  str(ep) + '_of_' + str(epochs) )\n",
    "    createDir(saveDir + '_' +  '_8.finalAccuracy--------' +  str(round(score[1],3)))\n",
    "    #save history\n",
    "    print(\"Saving history...\")\n",
    "    hist = model.history.history\n",
    "    saveName = 'fitHistory'\n",
    "    save_obj(saveDir, saveName, hist)\n",
    "\n",
    "    #save weights\n",
    "    print(\"Saving weights...\")\n",
    "    fileName = saveDir + 'weights.hdf5'\n",
    "    askAbortIfPathExists(fileName)\n",
    "    model.save_weights(fileName)\n",
    "\n",
    "    #save figures\n",
    "    print(\"Saving figures...\")\n",
    "    acc = hist['acc']\n",
    "    loss = hist['loss']\n",
    "    val_acc = hist['val_acc']\n",
    "    val_loss = hist['val_loss']\n",
    "    x = [i for i in range(len(acc))]\n",
    "\n",
    "    # Create plots\n",
    "    f = plt.figure(figsize=(15,7))\n",
    "    ax = f.add_subplot(121)\n",
    "    ax2 = f.add_subplot(122)\n",
    "\n",
    "    # Plot Loss \n",
    "    ax.plot(x, loss, color='blue', label=\"Train\", linewidth = 1)\n",
    "    ax.plot(x, val_loss, color='deepskyblue', label=\"Validation\", linewidth = 1)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(bottom=0)\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Plot Accuracy\n",
    "    ax2.plot(x, acc, 'b-', label=\"Train\", linewidth = 1)\n",
    "    ax2.plot(x, val_acc, color = 'deepskyblue', label=\"Validation\", linewidth = 1)\n",
    "    ax.set\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracty')\n",
    "    ax2.legend()\n",
    "    ax2.set_ylim(top=1)\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # Save plots\n",
    "    plt.savefig(saveDir + 'performance')\n",
    "    plt.show();\n",
    "\n",
    "    #save summary\n",
    "    print(\"Saving summary...\")\n",
    "    from contextlib import redirect_stdout\n",
    "    fileName = saveDir + 'modelsummary.txt'\n",
    "    askAbortIfPathExists(fileName)\n",
    "    with open(fileName, 'w') as f:\n",
    "        with redirect_stdout(f):\n",
    "            model.summary()\n",
    "            \n",
    "    # Save tensorboard logs\n",
    "    print(\"Saving tensorboard logs...\")\n",
    "    saveDir = 'Results/' + str(resID) + '/' + logDir[7:]\n",
    "    logDir = logDir \n",
    "    copyDirectory(logDir, saveDir)\n",
    "    \n",
    "    # Calc and save total time\n",
    "    saveDir = 'Results/' + str(resID) + '/'\n",
    "    with open(saveDir + 'fitTemp/startTime.txt', 'r') as file:\n",
    "        startTime = float(file.read())\n",
    "    endTime = time.time()\n",
    "    totalTime = endTime - startTime\n",
    "    if totalTime >3600*24:\n",
    "        totalTime = str(round(totalTime/(3600*24), 3)) + ' days'\n",
    "    elif totalTime >3600:\n",
    "        totalTime = str(round(totalTime/(3600), 3)) + ' hours'\n",
    "    elif totalTime >60:\n",
    "        totalTime = str(round(totalTime/(60), 3)) + ' minutes'\n",
    "    else:\n",
    "        totalTime = str(round(totalTime, 3)) + ' seconds'\n",
    "    createDir(saveDir + '_' +  '_10.totalTime-----------' +  str(totalTime))\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    print(\"All done saving stuff!\")\n",
    "\n",
    "##############################\n",
    "#\n",
    "#    COMPARE RESULTS\n",
    "#\n",
    "##############################\n",
    "def compareResults(res1, res2, label1 = '', label2 = '', metric1 = 'acc', metric2 = 'acc', saveFigName = '', makeEqual = False):\n",
    "    # Reshape input vector to fit on graph\n",
    "    def makeEqualLength(vec1, vec2):\n",
    "        l1 = len(vec1)\n",
    "        l2 = len(vec2)\n",
    "        if l1 == l2:\n",
    "             pass\n",
    "        elif l1 > l2: \n",
    "            l = l1 - l2\n",
    "            tail = np.ones((l), dtype = int) * vec2[-1]\n",
    "            vec2 = np.hstack((vec2,tail))\n",
    "        else:\n",
    "            l = l2 - l1\n",
    "            tail = np.ones((l), dtype = int) * vec1[-1]\n",
    "            vec1 = np.hstack((vec1,tail))\n",
    "        return vec1, vec2\n",
    "     \n",
    "    y1 = load_obj('Results/' + res1,'fitHistory')\n",
    "    y2 = load_obj('Results/' + res2,'fitHistory')\n",
    "    acc1 = y1[metric1]\n",
    "    acc2 = y2[metric2]\n",
    "    \n",
    "    if makeEqual:\n",
    "        acc1, acc2 = makeEqualLength(acc1, acc2)\n",
    "    \n",
    "    if label1 == '' :\n",
    "        label1 = res1\n",
    "    if label2 == '' :\n",
    "        label2 = res2\n",
    "        \n",
    "    bottom, top = plt.ylim()  # return the current ylim\n",
    "    if \"acc\" in metric1:\n",
    "        print('plotting accuracy')\n",
    "        yname = \"Accuracy\"\n",
    "        plt.ylim(bottom = bottom, top=1)\n",
    "    else:\n",
    "        print('plotting loss')\n",
    "        plt.ylim(bottom = 0, top=top)\n",
    "        yname = \"Loss\"\n",
    "    \n",
    "    x = [i for i in range(len(acc1))]\n",
    "    plt.plot(x,acc1, label = label1)\n",
    "    x = [i for i in range(len(acc2))]\n",
    "    plt.plot(x,acc2, label = label2)\n",
    "    bottom, top = plt.ylim()  # return the current ylim\n",
    "    if \"acc\" in metric1:\n",
    "        print('plotting accuracy')\n",
    "        yname = \"Accuracy\"\n",
    "        plt.ylim(bottom = bottom, top=1)\n",
    "    else:\n",
    "        print('plotting loss')\n",
    "        plt.ylim(bottom = 0, top=top)\n",
    "        yname = \"Loss\"\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel(yname)\n",
    "    plt.legend()\n",
    "    if saveFigName != '': plt.savefig(saveFigName)\n",
    "    plt.show()\n",
    "\n",
    "#compareResults('005','011', label1='test1', label2='test2', metric1='loss', metric2='loss', saveFigName = 'testmynd', makeEqual = True)\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    "#\n",
    "#    PRINT PROGRESS\n",
    "#\n",
    "##############################\n",
    "\n",
    "#             START      LAST PRINT             NOW              END\n",
    "#\n",
    "# Location:   0          i1                     i2               i3 = l\n",
    "#             ├──────────┼──────────────────────┼────────────────┤\n",
    "#             │          |<--- iSincePrint ---->|                │\n",
    "#             │          |                      |<---- iLeft --->│\n",
    "#             ├──────────┼──────────────────────┼────────────────┤\n",
    "#             │<------- timeElapsed ----------->|                │\n",
    "#             │          |<-- timeSincePrint -->|                │\n",
    "#             │          |                      |<-- timeLeft -->│\n",
    "#             ├──────────┼──────────────────────┼────────────────┤\n",
    "# Time:       t0         t1                    t2                T\n",
    "\n",
    "def printProgress(t0,t1,t2, i1, i2, l):\n",
    "    progress = 100*i2/l\n",
    "    timeElapsed = t2 - t0\n",
    "    timeSincePrint = t2 - t1\n",
    "    iSincePrint = i2 - i1\n",
    "    iLeft = l - i2\n",
    "    timePerI = timeSincePrint / iSincePrint\n",
    "    timeLeft = timePerI * iLeft\n",
    "\n",
    "    progressString = \"Progress: \" + str(\"%.3f\" % round(progress,3))\\\n",
    "                     + '%. Runtime: ' + str(formatTime(timeElapsed))\\\n",
    "                     + \" Time left: \" + str(formatTime(timeLeft))\\\n",
    "                     + \" At state: %.3e\" %Decimal(i2)\\\n",
    "                     + \"/ %.3e\" %Decimal(l)\n",
    "\n",
    "    sys.stdout.write(\"\\r\" + progressString)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def formatTime(t):\n",
    "    if t > 3600*24:\n",
    "        T = str(\"%.3f\" % float(round(t/3600/24,3))) + ' days.'\n",
    "    elif t > 3600:\n",
    "        T = str(\"%.3f\" % float(round(t/3600,3))) + ' hours.'\n",
    "    elif t > 60:\n",
    "        T = str(\"%.3f\" % float(round(t/60,3))) + ' minutes.'\n",
    "    else:\n",
    "        T = str(\"%.3f\" % float(round(t,3))) + ' seconds.'\n",
    "    return T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING TEMPLATE CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TRAINING TEMPLATE CODE\n",
    "\n",
    "##############################\n",
    "#\n",
    "#    PARAMETERS \n",
    "#\n",
    "##############################\n",
    "import math\n",
    "\n",
    "# Experiment description\n",
    "expDescr = \"Experiment description text\"\n",
    "\n",
    "# What data to use\n",
    "tableBase = '4PpKk'\n",
    "convertStates = False\n",
    "fractionOfDataToUse = 1 # [0,1]\n",
    "\n",
    "# Interactive (just in general if one is asked for confirmations, set to False if on autopilot over night f.x.)\n",
    "askForConfirmation = True\n",
    "\n",
    "# Transfer Learning\n",
    "loadWeights = False \n",
    "weightsSource = '024'\n",
    "loadCheckpointWeights = False\n",
    "\n",
    "# Compare with other result during training\n",
    "compareResultsDuringTraining = False\n",
    "compareWith = '013' # orginal net structure, trained from random on 4pc dataset\n",
    "\n",
    "\n",
    "# NN parameters\n",
    "# filters = [8,16,16,32,32]    #016:0.913  10kpm 2048:30                  8192:23:38% 32768:17:52% \n",
    "# filters = [8,16,32,64,128]   #005:0.952  50kpm 2048:37s    4096:28:50% \n",
    "# filters = [8,32,64,128,256]  #013:0.968 188kpm 2048:50s    4096:40s:61%             32768:46s:80% 65536:42s:99% \n",
    "# filters = [32,64,128,160,256]#014:0.974 388kpm 2048:3m:91% \n",
    "# filters = [16,32,64,128,128,128]#035:0.975 191kpm 2048:45s:50%/50% 2048:68s:78% \n",
    "# filters = [16,16,32,32,64,64,128] #054 70kpm\n",
    "# filters = [16,16,32,32,64,64,128] #054 70kpm\n",
    "filters = [16,32,32,64,128,128,128]\n",
    "filterShape = [2,2,2,2,2,2,2]\n",
    "batch_size = 256\n",
    "epochs = 150\n",
    "multiGPU = False\n",
    "whichGPU = 0\n",
    "# optimizer = 'Adam'\n",
    "optimizer = 'Adadelta'\n",
    "useBatchNorm = False\n",
    "\n",
    "# Other paramters\n",
    "confirmDirOverwrite = False\n",
    "tPrintInterval = 0.5 # for print progress\n",
    "yieldSize = 10000 # for load generator\n",
    "\n",
    "### NO NEED TO MODIFY BELOW ###\n",
    "# Generate dataset variables\n",
    "fileName = tableBase + '.hdf5'\n",
    "dataSetName = tableBase + '_onlyLegal'\n",
    "if not convertStates: \n",
    "    dataSetName = tableBase + '_onlyLegal_fullStates'\n",
    "dataSetWdlName = tableBase + '_Wdl_onlyLegal_3Values'\n",
    "\n",
    "# Number of Pieces\n",
    "nPi =  int(dataSetName[0])\n",
    "nPa = nPi - 2\n",
    "nWPa = math.ceil(nPa/2)\n",
    "\n",
    "# Select which GPU to use\n",
    "# if(multiGPU):\n",
    "#     from keras.utils.training_utils import multi_gpu_model\n",
    "# else:\n",
    "#     import os\n",
    "#     os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "#     # The GPU id to use, usually either \"0\" or \"1\"\n",
    "#     os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(whichGPU)\n",
    "    \n",
    "# Other NN stuff\n",
    "num_classes = 3\n",
    "input_shape = (4,8,8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model\n",
      "layer 0\n",
      "layer 1\n",
      "layer 2\n",
      "layer 3\n",
      "layer 4\n",
      "layer 5\n",
      "layer 6\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 16, 7, 7)          272       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 6, 32)         928       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 5, 32)         4128      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 4, 64)         8256      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 3, 128)        32896     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 11, 2, 128)        65664     \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 10, 1, 128)        65664     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 3843      \n",
      "=================================================================\n",
      "Total params: 181,651\n",
      "Trainable params: 181,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Starting with random weights\n",
      "Done creating model\n"
     ]
    }
   ],
   "source": [
    "model, nnStr = createModel()\n",
    "resID = genNextResultsDir(model)\n",
    "X_train, X_test, y_train, y_test = loadData(test_size = 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### expand 4 from rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating score\n",
      "7361728/7361728 [==============================] - 464s 63us/step\n",
      "(74360, 4, 8, 8)\n",
      "Evaluated test loss: 1.0992638008536706\n",
      "Evaluated test accuracy: 0.2607405489580707\n"
     ]
    }
   ],
   "source": [
    "# fitHistory, logDir = trainModel(resID, model)\n",
    "score = calcScore(model)\n",
    "# saveTrainResults(resID, model, logDir)\n",
    "# compareResults('005','013', metric1='acc', metric2='acc', saveFigName = 'testmynd', makeEqual = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### expand 4 from 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 16, 7, 7)          272       \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 15, 6, 32)         928       \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 14, 5, 32)         4128      \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 13, 4, 64)         8256      \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 12, 3, 128)        32896     \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 11, 2, 128)        65664     \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 10, 1, 128)        65664     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 3843      \n",
      "=================================================================\n",
      "Total params: 181,651\n",
      "Trainable params: 181,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Loading weights from Results/103/weights.hdf5\n",
      "Done creating model\n",
      "X_train shape: (74360, 4, 8, 8)\n",
      "y_train shape: (74360, 1)\n",
      "X_test shape: (7361728, 4, 8, 8)\n",
      "y_test shape: (7361728, 1)\n",
      "74360 train samples\n",
      "7361728 test samples\n",
      "Done loading dataset\n",
      "Calculating score\n",
      "7361728/7361728 [==============================] - 352s 48us/step\n",
      "(74360, 4, 8, 8)\n",
      "Evaluated test loss: 6.830602870777442\n",
      "Evaluated test accuracy: 0.5710041175115408\n"
     ]
    }
   ],
   "source": [
    "loadWeights = True \n",
    "weightsSource = '103'\n",
    "model, nnStr = createModel()\n",
    "X_train, X_test, y_train, y_test = loadData(test_size = 0.99)\n",
    "score2 = calcScore(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### many batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = loadData()\n",
    "\n",
    "bs = [256,256,256,512,512,512,1024,1024,1024]\n",
    "\n",
    "for batch_size in bs:\n",
    "    expDescr = \"Testing batch size effect for 70kpm network (7 CNN layers) {}bs\".format(batch_size) \n",
    "    print(\"---------------------- batch size \", batch_size, '-------------------------------')\n",
    "    model, nnStr = createModel()\n",
    "    resID = genNextResultsDir(model)\n",
    "    fitHistory, logDir = trainModel(resID, model)\n",
    "    score = calcScore(model)\n",
    "    saveTrainResults(resID, model, logDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc score using big data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_22 (Conv2D)           (None, 16, 7, 7)          272       \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 15, 6, 32)         928       \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 14, 5, 32)         4128      \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 13, 4, 64)         8256      \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 12, 3, 128)        32896     \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 11, 2, 128)        65664     \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 10, 1, 128)        65664     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 3843      \n",
      "=================================================================\n",
      "Total params: 181,651\n",
      "Trainable params: 181,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Loading weights from Results/117/weights.hdf5\n",
      "Done creating model\n",
      "Calculating score\n",
      "Progress: 100.000%. Runtime: 6.391 minutes. Time left: 0.000 seconds. At state: 7.430e+02/ 7.430e+02\n",
      "Evaluated loss: 0.04877576389672156\n",
      "Evaluated accuracy: 0.982710952737919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.04877576, 0.98271095])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadWeights = True \n",
    "weightsSource = '117'\n",
    "model, nnStr = createModel()\n",
    "fractionOfDataToUse = 1 # [0,1]\n",
    "calcScoreBigData(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc score using all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (7436088, 4, 8, 8)\n",
      "y_train shape: (7436088, 1)\n",
      "X_test shape: (0, 4, 8, 8)\n",
      "y_test shape: (0, 1)\n",
      "7436088 train samples\n",
      "0 test samples\n",
      "Done loading dataset\n",
      "7436088/7436088 [==============================] - 358s 48us/step\n",
      "[0.0488300708354827, 0.9840315230266237]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = loadData(test_size=0)\n",
    "score = model.evaluate(X_train, y_train, verbose=1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc score of 5pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TRAINING TEMPLATE CODE\n",
    "\n",
    "##############################\n",
    "#\n",
    "#    PARAMETERS \n",
    "#\n",
    "##############################\n",
    "import math\n",
    "\n",
    "# Experiment description\n",
    "expDescr = \"Experiment description text\"\n",
    "\n",
    "# What data to use\n",
    "tableBase = '5PPpKk'\n",
    "convertStates = False\n",
    "fractionOfDataToUse = 1 # [0,1]\n",
    "\n",
    "# Interactive (just in general if one is asked for confirmations, set to False if on autopilot over night f.x.)\n",
    "askForConfirmation = True\n",
    "\n",
    "# Transfer Learning\n",
    "loadWeights = False \n",
    "weightsSource = '024'\n",
    "loadCheckpointWeights = False\n",
    "\n",
    "# Compare with other result during training\n",
    "compareResultsDuringTraining = False\n",
    "compareWith = '013' # orginal net structure, trained from random on 4pc dataset\n",
    "\n",
    "\n",
    "# NN parameters\n",
    "# filters = [8,16,16,32,32]    #016:0.913  10kpm 2048:30                  8192:23:38% 32768:17:52% \n",
    "# filters = [8,16,32,64,128]   #005:0.952  50kpm 2048:37s    4096:28:50% \n",
    "# filters = [8,32,64,128,256]  #013:0.968 188kpm 2048:50s    4096:40s:61%             32768:46s:80% 65536:42s:99% \n",
    "# filters = [32,64,128,160,256]#014:0.974 388kpm 2048:3m:91% \n",
    "# filters = [16,32,64,128,128,128]#035:0.975 191kpm 2048:45s:50%/50% 2048:68s:78% \n",
    "# filters = [16,16,32,32,64,64,128] #054 70kpm\n",
    "# filters = [16,16,32,32,64,64,128] #054 70kpm\n",
    "filters = [16,32,32,64,128,128,128]\n",
    "filterShape = [2,2,2,2,2,2,2]\n",
    "batch_size = 256\n",
    "epochs = 150\n",
    "multiGPU = False\n",
    "whichGPU = 0\n",
    "# optimizer = 'Adam'\n",
    "optimizer = 'Adadelta'\n",
    "useBatchNorm = False\n",
    "\n",
    "# Other paramters\n",
    "confirmDirOverwrite = False\n",
    "tPrintInterval = 0.5 # for print progress\n",
    "yieldSize = 10000 # for load generator\n",
    "\n",
    "### NO NEED TO MODIFY BELOW ###\n",
    "# Generate dataset variables\n",
    "fileName = tableBase + '.hdf5'\n",
    "dataSetName = tableBase + '_onlyLegal'\n",
    "if not convertStates: \n",
    "    dataSetName = tableBase + '_onlyLegal_fullStates'\n",
    "dataSetWdlName = tableBase + '_Wdl_onlyLegal_3Values'\n",
    "\n",
    "# Number of Pieces\n",
    "nPi =  int(dataSetName[0])\n",
    "nPa = nPi - 2\n",
    "nWPa = math.ceil(nPa/2)\n",
    "\n",
    "# Select which GPU to use\n",
    "# if(multiGPU):\n",
    "#     from keras.utils.training_utils import multi_gpu_model\n",
    "# else:\n",
    "#     import os\n",
    "#     os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "#     # The GPU id to use, usually either \"0\" or \"1\"\n",
    "#     os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(whichGPU)\n",
    "    \n",
    "# Other NN stuff\n",
    "num_classes = 3\n",
    "input_shape = (4,8,8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 16, 7, 7)          272       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 6, 32)         928       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 5, 32)         4128      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 4, 64)         8256      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 3, 128)        32896     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 11, 2, 128)        65664     \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 10, 1, 128)        65664     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 3843      \n",
      "=================================================================\n",
      "Total params: 181,651\n",
      "Trainable params: 181,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Starting with random weights\n",
      "Done creating model\n",
      "Calculating score\n",
      "Progress: 20.009%. Runtime: 27.501 minutes. Time left: 1.816 hours. At state: 3.220e+03/ 1.609e+04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.000%. Runtime: 2.291 hours. Time left: 0.000 seconds. At state: 1.609e+04/ 1.609e+04\n",
      "Evaluated loss: 1.0994477336576423\n",
      "Evaluated accuracy: 0.1831222705768198\n",
      "[1.09944773 0.18312227]\n"
     ]
    }
   ],
   "source": [
    "loadWeights = False \n",
    "# weightsSource = '117'\n",
    "model, nnStr = createModel()\n",
    "fractionOfDataToUse = 1 # [0,1]\n",
    "scoreFromRandom = calcScoreBigData(model)\n",
    "print(scoreFromRandom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:endnetGpu]",
   "language": "python",
   "name": "conda-env-endnetGpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
