{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = '3PKk.hdf5'\n",
    "# fileName = 'AllStates_intVec.hdf5'\n",
    "nPi = 3\n",
    "nPa = 1\n",
    "nWPa = 1\n",
    "input_shape = (4,8,8)\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "dataDiv = 1\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "import keras\n",
    "# from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "# import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage\n",
      "    Open file handle:        f = h5py.File(fileName,'a')\n",
      "    Close file handle:       f.close\n",
      "    Load dataset :           dataset = loadDataSetFromFile(fileHandle, dataSetName)\n",
      "    Load dataset interactively:dataset = loadDataSetInteractive(f)\n",
      "    Display this help:       help() \n"
     ]
    }
   ],
   "source": [
    "# %load h5View.py\n",
    "import sys\n",
    "import h5py\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import signal\n",
    "\n",
    "def db(t,v):\n",
    "    if debug == True:\n",
    "        print(t,v)\n",
    "debug = False\n",
    "debug = True\n",
    "\n",
    "def Db(t,v):\n",
    "    if Debug == True:\n",
    "        print(t,v)\n",
    "Debug = False\n",
    "Debug = True\n",
    "\n",
    "def loadDataSetFromFile(fileName, dataSetName):\n",
    "    f = h5py.File(fileName, 'a')\n",
    "    return f[dataSetName]\n",
    "\n",
    "def loadDataSetFromHandle(f, dataSetName = None):\n",
    "    ####################\n",
    "    #\n",
    "    #   Signal Handler\n",
    "    #\n",
    "    ####################\n",
    "    def exit_gracefully(signum, frame):\n",
    "        # restore the original signal handler as otherwise evil things will happen\n",
    "        # in raw_input when CTRL+C is pressed, and our signal handler is not re-entrant\n",
    "        signal.signal(signal.SIGINT, original_sigint)\n",
    "\n",
    "        try:\n",
    "            # if input(\"\\nReally quit? (y/n)> \").lower().startswith('y'):\n",
    "            #     print(\"Flushing data to disk\")\n",
    "            #     f.close()\n",
    "            #     sys.exit(1)\n",
    "            print(\"\\nFlushing data to disk\")\n",
    "            f.close()\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nOk ok, quitting\")\n",
    "            print(\"Flushing data to disk\")\n",
    "            f.close()\n",
    "            sys.exit(1)\n",
    "\n",
    "        # restore the exit gracefully handler here\n",
    "        signal.signal(signal.SIGINT, exit_gracefully)\n",
    "\n",
    "    # store the original SIGINT handler\n",
    "    original_sigint = signal.getsignal(signal.SIGINT)\n",
    "    signal.signal(signal.SIGINT, exit_gracefully)\n",
    "\n",
    "    ####################\n",
    "    #\n",
    "    #   Main\n",
    "    #\n",
    "    ####################\n",
    "\n",
    "    # f = h5py.File(fileName, 'a')\n",
    "    # print(\"File loadded, f = \" + fileName)\n",
    "    \n",
    "    if dataSetName != None:\n",
    "    \n",
    "        print(\"Datasets:\")\n",
    "        i = 0\n",
    "        datasetNames = [ds for ds in f]\n",
    "        for ds in datasetNames:\n",
    "            print('Nr:' , i , '  Name:', ds.ljust(15), \"Size:\", f[ds].shape )\n",
    "            i += 1\n",
    "        # print(f[0])\n",
    "        while True:\n",
    "            selectDS = int(input(\"Select dataset: \"))\n",
    "            action = input(str(\"Do you want to load dataset: \" + datasetNames[selectDS] + \"? [y/n] \"))\n",
    "            if action == 'y' or action == 'Y':\n",
    "                dataset = f[datasetNames[selectDS]]\n",
    "                print(\"Dataset loaded\")\n",
    "                break\n",
    "        return dataset\n",
    "    else:\n",
    "        return f[dataSetName]\n",
    "\n",
    "def checkForTens(dataset):\n",
    "    d = 1000\n",
    "    l = len(dataset)\n",
    "    p = int(l/d)\n",
    "\n",
    "    for i in range(l):\n",
    "        if dataset[i]==10:\n",
    "            print('i: ', i, ' dataset[{}]'.format(i), dataset[i])\n",
    "        if i%p==0:\n",
    "            print(round(i/l,3))\n",
    "\n",
    "def help():\n",
    "\n",
    "    Db(\"Usage\" +\n",
    "       \"\\n    \" +\n",
    "       \"Open file handle:\".ljust(25) + \"f = h5py.File(fileName,'a')\" +\n",
    "       \"\\n    \" +\n",
    "       \"Close file handle:\".ljust(25) + \"f.close\" +\n",
    "       \"\\n    \" +\n",
    "       \"Load dataset :\".ljust(25) + \"dataset = loadDataSetFromFile(fileHandle, dataSetName)\" +\n",
    "       \"\\n    \" +\n",
    "       \"Load dataset interactively:\".ljust(25) + \"dataset = loadDataSetInteractive(f)\" +\n",
    "       \"\\n    \" +\n",
    "       \"Display this help:\". ljust(25) + \"help()\"\n",
    "       ,'')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ########################\n",
    "    #\n",
    "    #   Parameters\n",
    "    #\n",
    "    ########################\n",
    "\n",
    "#     fileName = 'AllStates_intVec.hdf5'\n",
    "#     dataSetName = \"36KPvKPP\"\n",
    "\n",
    "    help()\n",
    "\n",
    "#     db(\"\\nOpening file:\", fileName)\n",
    "#     f = h5py.File(fileName, 'a')\n",
    "#     dataset = 0\n",
    "#     dataset = loadDataSetInteractive(f)\n",
    "#     print(\"...to variable dataset\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "```\n",
    "3PKk-Wdl-Buffered == 3PKk-Wdl-Seq\n",
    "\n",
    "4PpKk-Wdl == 4PpKk-Wdl-Buffered != 4PpKk-Wdl-retry\n",
    "\n",
    "5PPpKk-Wdl != 5PPpKk-Wdl-Buffered\n",
    "    from 50.000 then buffered != 10, but wdl == 10\n",
    "```\n",
    "  \n",
    "### WDL score count\n",
    "#### 3PKk-WDL-Seq\n",
    "[0, 0, 125024, 0, 124960]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for vector to full state conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sq2hnit(sq):\n",
    "    col = sq%8\n",
    "    row = (sq - col)//8\n",
    "    return col,row\n",
    "\n",
    "# 0: pawns\n",
    "# 1: kings\n",
    "def vecSt2fullSt(vecSt, nPi, nPa, nWPa):\n",
    "    fullSt = np.zeros((4,8,8), dtype = 'int8')\n",
    "    for i in range(nPi - 2):\n",
    "        sq = vecSt[i]\n",
    "        col,row = sq2hnit(sq)\n",
    "        if i < nWPa:\n",
    "            fullSt[0][row][col] = 1\n",
    "        else:\n",
    "            fullSt[1][row][col] = -1\n",
    "    col,row = sq2hnit(vecSt[-2])\n",
    "    fullSt[2][row][col] = 1\n",
    "    col,row = sq2hnit(vecSt[-1])\n",
    "    fullSt[3][row][col] = -1\n",
    "    return fullSt \n",
    "\n",
    "def vecSt2fullSt_8x8x2(vecSt, nPi, nPa, nWPa):\n",
    "    fullSt = np.zeros((8,8,2), dtype = 'int8')\n",
    "    for i in range(nPi - 2):\n",
    "        sq = vecSt[i]\n",
    "        col,row = sq2hnit(sq)\n",
    "        if i < nWPa:\n",
    "            fullSt[row][col][0] = 1\n",
    "        else:\n",
    "            fullSt[row][col][0] = -1\n",
    "    col,row = sq2hnit(vecSt[-2])\n",
    "    fullSt[row][col][1] = 1\n",
    "    col,row = sq2hnit(vecSt[-1])\n",
    "    fullSt[row][col][1] = -1\n",
    "    return fullSt \n",
    "# nPi = 3\n",
    "# nPa = 1\n",
    "# nWPa = 1\n",
    "# vecSt2fullSt(d3[0], nPi, nPa, nWPa)\n",
    "# nPi = 5\n",
    "# nPa = 3\n",
    "# nWPa = 2\n",
    "# vecSt2fullSt(d5[0], nPi, nPa, nWPa)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "f = h5py.File(fileName, 'a')\n",
    "d3 = f['3PKk']\n",
    "d3t = f['3PKk-Wdl-Seq']\n",
    "# d5 = f['5PPpKk']\n",
    "l = len(d3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count nr of each score instance\n",
    "# wdlCounter placeholders: [-2, -1, 0, 1 ,2]\n",
    "\n",
    "def wdlCountingMachine(ds):\n",
    "    wdlCounter = [0,0,0,0,0]\n",
    "    l = len(ds)\n",
    "    i = 0\n",
    "    intv = l//100\n",
    "    for wdl in ds:\n",
    "        i += 1\n",
    "        if i%intv == 0:\n",
    "            sys.stdout.write(str((i*100)//l) + \" percentage\")\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.flush()\n",
    "        wdlCounter[wdl[0] + 2] += 1\n",
    "    print(wdlCounter)\n",
    "    return wdlCounter\n",
    "# wdlCountingMachine(d3t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([vecSt2fullSt(vecSt,nPi, nPa, nWPa) for vecSt in d3[:l//dataDiv]])\n",
    "y = d3t[:l//dataDiv]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82495, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (167489, 4, 8, 8)\n",
      "y_train shape: (167489, 1)\n",
      "X_test shape: (82495, 4, 8, 8)\n",
      "y_test shape: (82495, 1)\n",
      "167489 train samples\n",
      "82495 test samples\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 64, 6, 6)          2368      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 62, 4, 96)         5280      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 60, 2, 256)        221440    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 30720)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 153605    \n",
      "=================================================================\n",
      "Total params: 382,693\n",
      "Trainable params: 382,693\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=(3, 3),\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 data_format = \"channels_first\",\n",
    "#                  kernel_initializer = \n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(96, kernel_size=(3, 3),\n",
    "                 padding='valid',\n",
    "                 activation='relu'))\n",
    "model.add(Conv2D(256, kernel_size=(3, 3),\n",
    "                 padding='valid',\n",
    "                 activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 167489 samples, validate on 82495 samples\n",
      "Epoch 1/100\n",
      "  1664/167489 [..............................] - ETA: 24:24 - loss: 0.9884 - acc: 0.4892"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7b7995a62e27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           validation_data=(X_test, y_test))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/endnet/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/endnet/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/endnet/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/endnet/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/endnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
